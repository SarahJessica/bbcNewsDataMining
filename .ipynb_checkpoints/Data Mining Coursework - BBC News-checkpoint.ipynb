{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining on BBC News Topics\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Introduction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!** Name and describe the data set and include source: **2 marks**\n",
    "\n",
    "Import only UK Domestic BBC News stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1808"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allCc = pd.read_json('climate_change_data_subset.json')\n",
    "allTrump = pd.read_json('trump_data_subset.json')\n",
    "len(allCc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetId</th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>language</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38391034</td>\n",
       "      <td>/news/business-38391034</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>India's double first in climate battle</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2017-01-08T14:55:00+00:00</td>\n",
       "      <td>India opens two world-leading clean energy pro...</td>\n",
       "      <td>BBC News - India's double first in climate battle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38527710</td>\n",
       "      <td>/ukrainian/news-38527710</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>В Україну з півдня насуваються снігопади</td>\n",
       "      <td>uk</td>\n",
       "      <td>2017-01-06T06:31:56+00:00</td>\n",
       "      <td>Через циклон, який насувається з Чорного моря,...</td>\n",
       "      <td>BBC Україна - В Україну з півдня насуваються с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38600431</td>\n",
       "      <td>/news/uk-wales-38600431</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>£3m to protect Welsh and Irish coast from clim...</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2017-01-13T12:48:42+00:00</td>\n",
       "      <td>Coastal tourist sites most affected by climate...</td>\n",
       "      <td>BBC News - £3m to protect Welsh and Irish coas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38745937</td>\n",
       "      <td>/news/science-environment-38745937</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Defining a true 'pre-industrial' climate period</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2017-01-25T23:11:25+00:00</td>\n",
       "      <td>Scientists suggest pushing back a baseline fro...</td>\n",
       "      <td>BBC News - Defining a true 'pre-industrial' cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39053678</td>\n",
       "      <td>/news/science-environment-39053678</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Most wood energy schemes are a 'disaster' for ...</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2017-04-23T14:02:03+00:00</td>\n",
       "      <td>A new report says that using wood pellets to g...</td>\n",
       "      <td>BBC News - Most wood energy schemes are a 'dis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    assetId                            assetUri  \\\n",
       "0  38391034             /news/business-38391034   \n",
       "1  38527710            /ukrainian/news-38527710   \n",
       "2  38600431             /news/uk-wales-38600431   \n",
       "3  38745937  /news/science-environment-38745937   \n",
       "4  39053678  /news/science-environment-39053678   \n",
       "\n",
       "                                                body  \\\n",
       "0  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "1  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline language  \\\n",
       "0             India's double first in climate battle    en-gb   \n",
       "1           В Україну з півдня насуваються снігопади       uk   \n",
       "2  £3m to protect Welsh and Irish coast from clim...    en-gb   \n",
       "3    Defining a true 'pre-industrial' climate period    en-gb   \n",
       "4  Most wood energy schemes are a 'disaster' for ...    en-gb   \n",
       "\n",
       "               lastPublished  \\\n",
       "0  2017-01-08T14:55:00+00:00   \n",
       "1  2017-01-06T06:31:56+00:00   \n",
       "2  2017-01-13T12:48:42+00:00   \n",
       "3  2017-01-25T23:11:25+00:00   \n",
       "4  2017-04-23T14:02:03+00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "0  India opens two world-leading clean energy pro...   \n",
       "1  Через циклон, який насувається з Чорного моря,...   \n",
       "2  Coastal tourist sites most affected by climate...   \n",
       "3  Scientists suggest pushing back a baseline fro...   \n",
       "4  A new report says that using wood pellets to g...   \n",
       "\n",
       "                                               title  \n",
       "0  BBC News - India's double first in climate battle  \n",
       "1  BBC Україна - В Україну з півдня насуваються с...  \n",
       "2  BBC News - £3m to protect Welsh and Irish coas...  \n",
       "3  BBC News - Defining a true 'pre-industrial' cl...  \n",
       "4  BBC News - Most wood energy schemes are a 'dis...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allCc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetId</th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>language</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43597394</td>\n",
       "      <td>/portuguese/internacional-43597394</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>A pouco conhecida história de como os EUA leva...</td>\n",
       "      <td>pt-BR</td>\n",
       "      <td>2018-04-01T17:43:20+00:00</td>\n",
       "      <td>O governo americano foi o primeiro a violar o ...</td>\n",
       "      <td>BBC Brasil - A pouco conhecida história de com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43607394</td>\n",
       "      <td>/portuguese/internacional-43607394</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>O ativista que criou uma identidade falsa para...</td>\n",
       "      <td>pt-BR</td>\n",
       "      <td>2018-04-02T07:34:34+00:00</td>\n",
       "      <td>Um ativista antirracismo passou um ano infiltr...</td>\n",
       "      <td>BBC Brasil - O ativista que criou uma identida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43606088</td>\n",
       "      <td>/news/world-us-canada-43606088</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Donald Trump steps up attacks on Amazon</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2018-03-31T21:23:32+00:00</td>\n",
       "      <td>The president accuses the online retail giant ...</td>\n",
       "      <td>BBC News - Donald Trump steps up attacks on Am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43601557</td>\n",
       "      <td>/news/world-us-canada-43601557</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>US may tie social media to visa applications</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2018-03-31T12:55:24+00:00</td>\n",
       "      <td>A state department proposal could require visa...</td>\n",
       "      <td>BBC News - US may tie social media to visa app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43578462</td>\n",
       "      <td>/news/world-us-canada-43578462</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Is Trump ready for talks with North Korea?</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2018-03-30T21:20:28+00:00</td>\n",
       "      <td>What preparations is the White House making fo...</td>\n",
       "      <td>BBC News - Is Trump ready for talks with North...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    assetId                            assetUri  \\\n",
       "0  43597394  /portuguese/internacional-43597394   \n",
       "1  43607394  /portuguese/internacional-43607394   \n",
       "2  43606088      /news/world-us-canada-43606088   \n",
       "3  43601557      /news/world-us-canada-43601557   \n",
       "4  43578462      /news/world-us-canada-43578462   \n",
       "\n",
       "                                                body  \\\n",
       "0  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "1  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline language  \\\n",
       "0  A pouco conhecida história de como os EUA leva...    pt-BR   \n",
       "1  O ativista que criou uma identidade falsa para...    pt-BR   \n",
       "2            Donald Trump steps up attacks on Amazon    en-gb   \n",
       "3       US may tie social media to visa applications    en-gb   \n",
       "4         Is Trump ready for talks with North Korea?    en-gb   \n",
       "\n",
       "               lastPublished  \\\n",
       "0  2018-04-01T17:43:20+00:00   \n",
       "1  2018-04-02T07:34:34+00:00   \n",
       "2  2018-03-31T21:23:32+00:00   \n",
       "3  2018-03-31T12:55:24+00:00   \n",
       "4  2018-03-30T21:20:28+00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "0  O governo americano foi o primeiro a violar o ...   \n",
       "1  Um ativista antirracismo passou um ano infiltr...   \n",
       "2  The president accuses the online retail giant ...   \n",
       "3  A state department proposal could require visa...   \n",
       "4  What preparations is the White House making fo...   \n",
       "\n",
       "                                               title  \n",
       "0  BBC Brasil - A pouco conhecida história de com...  \n",
       "1  BBC Brasil - O ativista que criou uma identida...  \n",
       "2  BBC News - Donald Trump steps up attacks on Am...  \n",
       "3  BBC News - US may tie social media to visa app...  \n",
       "4  BBC News - Is Trump ready for talks with North...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Watch out! Looks like my Climate Change data set in in date order whereas my Trump data set is in reverse date order!\n",
    "\n",
    "**!** attribute and value domain discussion: **2 marks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = allCc[allCc.assetUri.str.startswith('/news')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = allTrump[allTrump.assetUri.str.startswith('/news')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3254"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3254 entries, 2 to 5133\n",
      "Data columns (total 8 columns):\n",
      "assetId          3254 non-null int64\n",
      "assetUri         3254 non-null object\n",
      "body             3250 non-null object\n",
      "headline         3254 non-null object\n",
      "language         3254 non-null object\n",
      "lastPublished    3254 non-null object\n",
      "summary          3254 non-null object\n",
      "title            3254 non-null object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 228.8+ KB\n"
     ]
    }
   ],
   "source": [
    "trump.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 664 entries, 0 to 1805\n",
      "Data columns (total 8 columns):\n",
      "assetId          664 non-null int64\n",
      "assetUri         664 non-null object\n",
      "body             662 non-null object\n",
      "headline         664 non-null object\n",
      "language         664 non-null object\n",
      "lastPublished    664 non-null object\n",
      "summary          664 non-null object\n",
      "title            664 non-null object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 46.7+ KB\n"
     ]
    }
   ],
   "source": [
    "cc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(trump['language'] != 'en-gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cc['language'] != 'en-gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = cc.drop(columns=['assetId','language'])\n",
    "trump = trump.drop(columns=['assetId','language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the article bodies are missing, so these will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = cc.dropna(axis=0, how='any')\n",
    "trump = trump.dropna(axis=0, how='any')\n",
    "cc.reset_index(drop=True)\n",
    "trump.reset_index(drop=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove XML tags from the body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup as bs\n",
    "# I started by using BeautifulSoup, however BeautifulSoup is for HTML whereas this is XML.\n",
    "# The BBC has it's own XML standard which is not entirely conventional, \n",
    "# therefore I found it easier to write my own text extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>cleanBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/news/world-us-canada-43606088</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Donald Trump steps up attacks on Amazon</td>\n",
       "      <td>2018-03-31T21:23:32+00:00</td>\n",
       "      <td>The president accuses the online retail giant ...</td>\n",
       "      <td>BBC News - Donald Trump steps up attacks on Am...</td>\n",
       "      <td>President Donald Trump has stepped up his atta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/news/world-us-canada-43601557</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>US may tie social media to visa applications</td>\n",
       "      <td>2018-03-31T12:55:24+00:00</td>\n",
       "      <td>A state department proposal could require visa...</td>\n",
       "      <td>BBC News - US may tie social media to visa app...</td>\n",
       "      <td>The Trump administration has said it wants to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/news/world-us-canada-43578462</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Is Trump ready for talks with North Korea?</td>\n",
       "      <td>2018-03-30T21:20:28+00:00</td>\n",
       "      <td>What preparations is the White House making fo...</td>\n",
       "      <td>BBC News - Is Trump ready for talks with North...</td>\n",
       "      <td>Never before has a US president prepared for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/news/world-us-canada-43591904</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump: Billions spent protecting other countri...</td>\n",
       "      <td>2018-03-29T19:31:50+00:00</td>\n",
       "      <td>Mr Trump says securing the Korean border comes...</td>\n",
       "      <td>BBC News - Trump: Billions spent protecting ot...</td>\n",
       "      <td>The US president says protecting the Korean bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/news/world-us-canada-43577444</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump loses bid to dismiss hotel lawsuit</td>\n",
       "      <td>2018-03-29T02:13:35+00:00</td>\n",
       "      <td>A judge allows a case to determine if the pres...</td>\n",
       "      <td>BBC News - Trump loses bid to dismiss hotel la...</td>\n",
       "      <td>Donald Trump's attempt to dismiss a lawsuit al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         assetUri  \\\n",
       "2  /news/world-us-canada-43606088   \n",
       "3  /news/world-us-canada-43601557   \n",
       "4  /news/world-us-canada-43578462   \n",
       "6  /news/world-us-canada-43591904   \n",
       "7  /news/world-us-canada-43577444   \n",
       "\n",
       "                                                body  \\\n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "6  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "7  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline  \\\n",
       "2            Donald Trump steps up attacks on Amazon   \n",
       "3       US may tie social media to visa applications   \n",
       "4         Is Trump ready for talks with North Korea?   \n",
       "6  Trump: Billions spent protecting other countri...   \n",
       "7           Trump loses bid to dismiss hotel lawsuit   \n",
       "\n",
       "               lastPublished  \\\n",
       "2  2018-03-31T21:23:32+00:00   \n",
       "3  2018-03-31T12:55:24+00:00   \n",
       "4  2018-03-30T21:20:28+00:00   \n",
       "6  2018-03-29T19:31:50+00:00   \n",
       "7  2018-03-29T02:13:35+00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "2  The president accuses the online retail giant ...   \n",
       "3  A state department proposal could require visa...   \n",
       "4  What preparations is the White House making fo...   \n",
       "6  Mr Trump says securing the Korean border comes...   \n",
       "7  A judge allows a case to determine if the pres...   \n",
       "\n",
       "                                               title  \\\n",
       "2  BBC News - Donald Trump steps up attacks on Am...   \n",
       "3  BBC News - US may tie social media to visa app...   \n",
       "4  BBC News - Is Trump ready for talks with North...   \n",
       "6  BBC News - Trump: Billions spent protecting ot...   \n",
       "7  BBC News - Trump loses bid to dismiss hotel la...   \n",
       "\n",
       "                                           cleanBody  \n",
       "2  President Donald Trump has stepped up his atta...  \n",
       "3  The Trump administration has said it wants to ...  \n",
       "4  Never before has a US president prepared for a...  \n",
       "6  The US president says protecting the Korean bo...  \n",
       "7  Donald Trump's attempt to dismiss a lawsuit al...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_text_into_new_column(df, column_name, new_column_name):\n",
    "    temp_column = []\n",
    "    for i in range(len(df)):\n",
    "        text = df[column_name].iloc[i]\n",
    "        tags = [\"assetId\", \"assetTypeCode\", \"categoryId\", \"categoryName\", \n",
    "                \"crosshead\", \"changeQueueId\", \"embed\", \"hasShortForm\", \"language\", \n",
    "                \"provider\", \"publicationStatus\", \"workerCallingCard\"]\n",
    "        for tag in tags:\n",
    "            tag_regex = '<'+ re.escape(tag) + '.*?' + re.escape(tag) + '>'\n",
    "            text = re.sub(tag_regex, '', text)\n",
    "        text = re.sub('<.*?>', '', text)\n",
    "        timestamp_regex = '\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+\\d{2}:\\d{2}'\n",
    "        text = re.sub(timestamp_regex, '', text)\n",
    "        text = re.sub('&amp;', '&', text)\n",
    "        text = re.sub('\\.\\.\\.', ' ', text)\n",
    "        whitespace_after_full_stop_regex = '(?<=[a-z])\\.(?=[A-Z])'\n",
    "        text = re.sub(whitespace_after_full_stop_regex, '. ', text)\n",
    "        whitespace_after_question_mark_regex = '(?<=[a-z])\\?(?=[A-Z])'\n",
    "        text = re.sub(whitespace_after_question_mark_regex, '. ', text)\n",
    "        temp_column.append(text)\n",
    "    df[new_column_name] = temp_column\n",
    "\n",
    "extract_text_into_new_column(cc, 'body', 'cleanBody')\n",
    "extract_text_into_new_column(trump, 'body', 'cleanBody')\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump.lastPublished = pd.to_datetime(trump.lastPublished)\n",
    "cc.lastPublished = pd.to_datetime(cc.lastPublished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all much of the data has been removed, the indexes need to be reset. The trump data (looking just by the head print out) appears to needs to be 'reversed' because the dates run in the opposite order. Both dataframes will be sorted in date order as they are reindexed because this is likely to make visualisation easier later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>cleanBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/news/world-us-canada-43606088</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Donald Trump steps up attacks on Amazon</td>\n",
       "      <td>2018-03-31 21:23:32</td>\n",
       "      <td>The president accuses the online retail giant ...</td>\n",
       "      <td>BBC News - Donald Trump steps up attacks on Am...</td>\n",
       "      <td>President Donald Trump has stepped up his atta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/news/world-us-canada-43601557</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>US may tie social media to visa applications</td>\n",
       "      <td>2018-03-31 12:55:24</td>\n",
       "      <td>A state department proposal could require visa...</td>\n",
       "      <td>BBC News - US may tie social media to visa app...</td>\n",
       "      <td>The Trump administration has said it wants to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/news/world-us-canada-43578462</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Is Trump ready for talks with North Korea?</td>\n",
       "      <td>2018-03-30 21:20:28</td>\n",
       "      <td>What preparations is the White House making fo...</td>\n",
       "      <td>BBC News - Is Trump ready for talks with North...</td>\n",
       "      <td>Never before has a US president prepared for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/news/world-us-canada-43591904</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump: Billions spent protecting other countri...</td>\n",
       "      <td>2018-03-29 19:31:50</td>\n",
       "      <td>Mr Trump says securing the Korean border comes...</td>\n",
       "      <td>BBC News - Trump: Billions spent protecting ot...</td>\n",
       "      <td>The US president says protecting the Korean bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/news/world-us-canada-43577444</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump loses bid to dismiss hotel lawsuit</td>\n",
       "      <td>2018-03-29 02:13:35</td>\n",
       "      <td>A judge allows a case to determine if the pres...</td>\n",
       "      <td>BBC News - Trump loses bid to dismiss hotel la...</td>\n",
       "      <td>Donald Trump's attempt to dismiss a lawsuit al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         assetUri  \\\n",
       "2  /news/world-us-canada-43606088   \n",
       "3  /news/world-us-canada-43601557   \n",
       "4  /news/world-us-canada-43578462   \n",
       "6  /news/world-us-canada-43591904   \n",
       "7  /news/world-us-canada-43577444   \n",
       "\n",
       "                                                body  \\\n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "6  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "7  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline       lastPublished  \\\n",
       "2            Donald Trump steps up attacks on Amazon 2018-03-31 21:23:32   \n",
       "3       US may tie social media to visa applications 2018-03-31 12:55:24   \n",
       "4         Is Trump ready for talks with North Korea? 2018-03-30 21:20:28   \n",
       "6  Trump: Billions spent protecting other countri... 2018-03-29 19:31:50   \n",
       "7           Trump loses bid to dismiss hotel lawsuit 2018-03-29 02:13:35   \n",
       "\n",
       "                                             summary  \\\n",
       "2  The president accuses the online retail giant ...   \n",
       "3  A state department proposal could require visa...   \n",
       "4  What preparations is the White House making fo...   \n",
       "6  Mr Trump says securing the Korean border comes...   \n",
       "7  A judge allows a case to determine if the pres...   \n",
       "\n",
       "                                               title  \\\n",
       "2  BBC News - Donald Trump steps up attacks on Am...   \n",
       "3  BBC News - US may tie social media to visa app...   \n",
       "4  BBC News - Is Trump ready for talks with North...   \n",
       "6  BBC News - Trump: Billions spent protecting ot...   \n",
       "7  BBC News - Trump loses bid to dismiss hotel la...   \n",
       "\n",
       "                                           cleanBody  \n",
       "2  President Donald Trump has stepped up his atta...  \n",
       "3  The Trump administration has said it wants to ...  \n",
       "4  Never before has a US president prepared for a...  \n",
       "6  The US president says protecting the Korean bo...  \n",
       "7  Donald Trump's attempt to dismiss a lawsuit al...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_by_date_and_reindex(df, date_column):\n",
    "    df = df.sort_values(by=[date_column])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "sort_by_date_and_reindex(cc, 'lastPublished')\n",
    "sort_by_date_and_reindex(trump, 'lastPublished')\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>cleanBody</th>\n",
       "      <th>bodyLen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/news/world-us-canada-43606088</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Donald Trump steps up attacks on Amazon</td>\n",
       "      <td>2018-03-31 21:23:32</td>\n",
       "      <td>The president accuses the online retail giant ...</td>\n",
       "      <td>BBC News - Donald Trump steps up attacks on Am...</td>\n",
       "      <td>President Donald Trump has stepped up his atta...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/news/world-us-canada-43601557</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>US may tie social media to visa applications</td>\n",
       "      <td>2018-03-31 12:55:24</td>\n",
       "      <td>A state department proposal could require visa...</td>\n",
       "      <td>BBC News - US may tie social media to visa app...</td>\n",
       "      <td>The Trump administration has said it wants to ...</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/news/world-us-canada-43578462</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Is Trump ready for talks with North Korea?</td>\n",
       "      <td>2018-03-30 21:20:28</td>\n",
       "      <td>What preparations is the White House making fo...</td>\n",
       "      <td>BBC News - Is Trump ready for talks with North...</td>\n",
       "      <td>Never before has a US president prepared for a...</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/news/world-us-canada-43591904</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump: Billions spent protecting other countri...</td>\n",
       "      <td>2018-03-29 19:31:50</td>\n",
       "      <td>Mr Trump says securing the Korean border comes...</td>\n",
       "      <td>BBC News - Trump: Billions spent protecting ot...</td>\n",
       "      <td>The US president says protecting the Korean bo...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/news/world-us-canada-43577444</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump loses bid to dismiss hotel lawsuit</td>\n",
       "      <td>2018-03-29 02:13:35</td>\n",
       "      <td>A judge allows a case to determine if the pres...</td>\n",
       "      <td>BBC News - Trump loses bid to dismiss hotel la...</td>\n",
       "      <td>Donald Trump's attempt to dismiss a lawsuit al...</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         assetUri  \\\n",
       "2  /news/world-us-canada-43606088   \n",
       "3  /news/world-us-canada-43601557   \n",
       "4  /news/world-us-canada-43578462   \n",
       "6  /news/world-us-canada-43591904   \n",
       "7  /news/world-us-canada-43577444   \n",
       "\n",
       "                                                body  \\\n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "6  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "7  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline       lastPublished  \\\n",
       "2            Donald Trump steps up attacks on Amazon 2018-03-31 21:23:32   \n",
       "3       US may tie social media to visa applications 2018-03-31 12:55:24   \n",
       "4         Is Trump ready for talks with North Korea? 2018-03-30 21:20:28   \n",
       "6  Trump: Billions spent protecting other countri... 2018-03-29 19:31:50   \n",
       "7           Trump loses bid to dismiss hotel lawsuit 2018-03-29 02:13:35   \n",
       "\n",
       "                                             summary  \\\n",
       "2  The president accuses the online retail giant ...   \n",
       "3  A state department proposal could require visa...   \n",
       "4  What preparations is the White House making fo...   \n",
       "6  Mr Trump says securing the Korean border comes...   \n",
       "7  A judge allows a case to determine if the pres...   \n",
       "\n",
       "                                               title  \\\n",
       "2  BBC News - Donald Trump steps up attacks on Am...   \n",
       "3  BBC News - US may tie social media to visa app...   \n",
       "4  BBC News - Is Trump ready for talks with North...   \n",
       "6  BBC News - Trump: Billions spent protecting ot...   \n",
       "7  BBC News - Trump loses bid to dismiss hotel la...   \n",
       "\n",
       "                                           cleanBody  bodyLen  \n",
       "2  President Donald Trump has stepped up his atta...      320  \n",
       "3  The Trump administration has said it wants to ...      623  \n",
       "4  Never before has a US president prepared for a...     1007  \n",
       "6  The US president says protecting the Korean bo...       20  \n",
       "7  Donald Trump's attempt to dismiss a lawsuit al...      330  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why this method?\n",
    "# though we are splitting on an empty space to get the word count. This method is likely to give a good enough value\n",
    "# because the paragraphs were marked with a <p> tag which has now been stripped out rather than a new line\n",
    "# there are more accurate ways of obtaining a word count, however this is accurate enough for \n",
    "# the initial investigation that is this coursework\n",
    "\n",
    "def create_column_containing_word_count(df, column_name, new_column_name):\n",
    "    temp_column = []\n",
    "    for i in range(len(df)):\n",
    "        current_body = len(df[column_name].iloc[i].split(' '))\n",
    "        temp_column.append(current_body)\n",
    "    df[new_column_name] = temp_column\n",
    "    \n",
    "create_column_containing_word_count(cc, 'cleanBody', 'bodyLen')\n",
    "create_column_containing_word_count(trump, 'cleanBody', 'bodyLen')\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>cleanBody</th>\n",
       "      <th>bodyLen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/news/business-38391034</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>India's double first in climate battle</td>\n",
       "      <td>2017-01-08 14:55:00</td>\n",
       "      <td>India opens two world-leading clean energy pro...</td>\n",
       "      <td>BBC News - India's double first in climate battle</td>\n",
       "      <td>Two world-leading clean energy projects have o...</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/news/uk-wales-38600431</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>£3m to protect Welsh and Irish coast from clim...</td>\n",
       "      <td>2017-01-13 12:48:42</td>\n",
       "      <td>Coastal tourist sites most affected by climate...</td>\n",
       "      <td>BBC News - £3m to protect Welsh and Irish coas...</td>\n",
       "      <td>Coastal tourist sites most affected by climate...</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/news/science-environment-38745937</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Defining a true 'pre-industrial' climate period</td>\n",
       "      <td>2017-01-25 23:11:25</td>\n",
       "      <td>Scientists suggest pushing back a baseline fro...</td>\n",
       "      <td>BBC News - Defining a true 'pre-industrial' cl...</td>\n",
       "      <td>Scientists are seeking to define a new baselin...</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/news/science-environment-39053678</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Most wood energy schemes are a 'disaster' for ...</td>\n",
       "      <td>2017-04-23 14:02:03</td>\n",
       "      <td>A new report says that using wood pellets to g...</td>\n",
       "      <td>BBC News - Most wood energy schemes are a 'dis...</td>\n",
       "      <td>Using wood pellets to generate low-carbon elec...</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/news/uk-39679584</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>March for Science: Thousands in London join gl...</td>\n",
       "      <td>2017-05-01 20:19:03</td>\n",
       "      <td>Organisers say the growth of fake news makes i...</td>\n",
       "      <td>BBC News - March for Science: Thousands in Lon...</td>\n",
       "      <td>Thousands of people have gathered in London to...</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             assetUri  \\\n",
       "0             /news/business-38391034   \n",
       "2             /news/uk-wales-38600431   \n",
       "3  /news/science-environment-38745937   \n",
       "4  /news/science-environment-39053678   \n",
       "8                   /news/uk-39679584   \n",
       "\n",
       "                                                body  \\\n",
       "0  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "8  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline       lastPublished  \\\n",
       "0             India's double first in climate battle 2017-01-08 14:55:00   \n",
       "2  £3m to protect Welsh and Irish coast from clim... 2017-01-13 12:48:42   \n",
       "3    Defining a true 'pre-industrial' climate period 2017-01-25 23:11:25   \n",
       "4  Most wood energy schemes are a 'disaster' for ... 2017-04-23 14:02:03   \n",
       "8  March for Science: Thousands in London join gl... 2017-05-01 20:19:03   \n",
       "\n",
       "                                             summary  \\\n",
       "0  India opens two world-leading clean energy pro...   \n",
       "2  Coastal tourist sites most affected by climate...   \n",
       "3  Scientists suggest pushing back a baseline fro...   \n",
       "4  A new report says that using wood pellets to g...   \n",
       "8  Organisers say the growth of fake news makes i...   \n",
       "\n",
       "                                               title  \\\n",
       "0  BBC News - India's double first in climate battle   \n",
       "2  BBC News - £3m to protect Welsh and Irish coas...   \n",
       "3  BBC News - Defining a true 'pre-industrial' cl...   \n",
       "4  BBC News - Most wood energy schemes are a 'dis...   \n",
       "8  BBC News - March for Science: Thousands in Lon...   \n",
       "\n",
       "                                           cleanBody  bodyLen  \n",
       "0  Two world-leading clean energy projects have o...      764  \n",
       "2  Coastal tourist sites most affected by climate...      242  \n",
       "3  Scientists are seeking to define a new baselin...      822  \n",
       "4  Using wood pellets to generate low-carbon elec...      991  \n",
       "8  Thousands of people have gathered in London to...      451  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(cc.lastPublished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(trump.lastPublished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining\n",
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/leives01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//TODO\n",
    "* Tokenize each article into sentences. Each sentence is called a token. \n",
    "* Remove stop words -- stop words are very common words that don't really give any extra information regarding the meaning of the text ('a','as','and' etc.).\n",
    "* Identify n-grams (bigrams)\n",
    "* stemming similar words e.g. close, closing and closed \n",
    "* Part of Speech (POS) tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text\n",
    "I start by breaking the `cleanBody` into sentences and tokenizing them which means removing common stop words such as 'the', 'and', 'a' and 'that'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# sent = sent_tokenize(cc['cleanBody'].iloc[0])\n",
    "\n",
    "# sent = \"the cat sat on the mat. The cat did this. The cat did that.\"\n",
    "# print(sent_tokenize(sent))\n",
    "\n",
    "# matrix = []\n",
    "# for i in range(len(cc)):\n",
    "#     current = cc['cleanBody'].iloc[0]\n",
    "#     sent = sent_tokenize(current)\n",
    "#     matrix.append(sent)\n",
    "    \n",
    "# print(matrix)\n",
    "\n",
    "def make_matrix_of_sentences(df, column):\n",
    "    matrix = []\n",
    "    for i in range(len(df)):\n",
    "        current = df[column].iloc[i]\n",
    "        sent = sent_tokenize(current)\n",
    "        matrix.append(sent)\n",
    "    return matrix\n",
    "\n",
    "ccMatrix = make_matrix_of_sentences(cc, 'cleanBody')\n",
    "trumpMatrix = make_matrix_of_sentences(trump, 'cleanBody')\n",
    "# ccMatrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Australia's Great Barrier Reef can be saved only if urgent steps are taken to reduce global warming, new research has warned.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccMatrix[100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Australia\\'s Great Barrier Reef can be saved only if urgent steps are taken to reduce global warming, new research has warned. Attempting to stop coral bleaching through any other method will not be sufficient, according to scientists. The research, published in the journal Nature, said bleaching events should no longer be studied individually, but as threats to the reef\\'s survival. The bleaching - or loss of algae - in 2016 was the worst on record.\"Climate change is the single greatest threat to the Great Barrier Reef,\" said co-author Prof Morgan Pratchett, from Queensland\\'s James Cook University.\"It all comes down to what the governments in Australia and around the world do in terms of mitigating further rises in temperatures.\"     What causes coral bleaching?        Coral bleaching is caused by rising water temperatures resulting from two natural warm currents.    It is exacerbated by man-made climate change, as the oceans are absorbing about 93% of the increase in the Earth\\'s heat.    Bleaching happens when corals under stress drive out the algae known as zooxanthellae that give them colour.    If normal conditions return, the corals can recover, but it can take decades, and if the stress continues the corals can die.  Lead author Prof Terry Hughes warned bleaching events had become \"the new normal\".Last week, he said an aerial survey had shown evidence of mass bleaching in consecutive summers for the first time. The scale of the damage will be examined in the next three weeks by the National Coral Bleaching Taskforce, a collaboration of scientists and reef managers.      Australian summer broke 205 records        Can super coral save the Great Barrier Reef?    Is the Great Barrier Reef coming to the end of its life?  Prof Pratchett said he remained optimistic the reef could recover, but the \"window of opportunity\" to curb emissions was closing. \"It\\'s the number one thing we need to think about now to save the reef,\" he told the BBC.Improving fishing practices or water quality would not be enough, he said.    The second bleaching is causing concerns over the reef\\'s long term health.  The reef - a vast collection of thousands of smaller coral reefs stretching from the northern tip of Queensland to the state\\'s southern city of Bundaberg - was given World Heritage status in 1981.The UN says it is the \"most biodiverse\" of all the World Heritage sites, and of \"enormous scientific and intrinsic importance\".'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc['cleanBody'].iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punkt includes stopwords in different languages\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "custom_stop_words = [\"'s\", \"``\", \"''\", \"\\\"\"]\n",
    "stop_words = set(stopwords.words('english') + list(punctuation) + list(custom_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(df, column):\n",
    "    words_matrix = []\n",
    "    for i in range (len(df)):\n",
    "        current = df[column].iloc[i]\n",
    "        these_words = [word for word in word_tokenize(current) if word not in stop_words]\n",
    "        these_words\n",
    "        words_matrix.append(these_words)\n",
    "    return words_matrix\n",
    "\n",
    "ccTokens = tokenize_words(cc, 'cleanBody')\n",
    "trumpTokens = tokenize_words(trump, 'cleanBody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['President', 'Donald', 'Trump', \"'Nambia\", 'Africa', \"'business\", 'potential', 'Nambia', 'health', 'system', 'increasingly', 'self-sufficient', 'said', 'US', 'President', 'Donald', 'Trump', 'lunch', 'African', 'leaders', 'New', 'York', 'Wednesday', 'reeling', 'list', 'nations', 'achievements', 'But', 'country', 'exists', 'Could', 'US', 'leader', 'referring', 'Namibia', 'Zambia', 'Or', 'perhaps', 'The', 'Gambia', 'Social', 'media', 'users', 'wasted', 'time', 'offering', 'explanations', 'One', 'person', 'shared', 'image', 'US', 'race', 'activist', 'Rachel', 'Dolezal', 'identifies', 'black', 'despite', 'born', 'white', 'recently', 'visited', 'South', 'Africa', 'Find', 'Namibia', 'Zambia', 'The', 'Gambia', 'Namibia', 'country', 'profileNamibia', 'country', 'profileProvides', 'overview', 'Namibia', 'including', 'key', 'events', 'facts', 'sparsely', 'populated', 'stable', 'country', 'Africa', 'south-west', 'coast', 'BBC', 'News', 'Namibia', 'country', 'profileZambia', 'country', 'profileZambia', 'profileProvides', 'overview', 'key', 'facts', 'events', 'timelines', 'leader', 'profiles', 'along', 'current', 'news', 'Zambia', 'BBC', 'News', 'Zambia', 'country', 'profileThe', 'Gambia', 'country', 'profileThe', 'Gambia', 'profileProvides', 'overview', 'key', 'facts', 'events', 'timelines', 'leader', 'profiles', 'along', 'current', 'news', 'The', 'Gambia', 'BBC', 'News', 'The', 'Gambia', 'country', 'profileAfrica', 'Live', 'More', 'storiesBBC', 'Africa', 'Live', 'pageNamibia', 'President', 'Hage', 'Geingob', 'present', 'Mr', 'Trump', 'made', 'gaffe', 'He', 'yet', 'commented', 'Some', 'Twitter', 'users', 'also', 'bristled', 'President', 'Trump', 'comments', 'Africa', 'tremendous', 'business', 'potential', 'I', 'many', 'friends', 'going', 'countries', 'trying', 'get', 'rich', 'I', 'congratulate', \"'re\", 'spending', 'lot', 'money', 'Mr', 'Trump', 'said', 'It', 'represents', 'huge', 'amounts', 'different', 'markets', 'It', 'really', 'become', 'place', 'go', 'want', 'go', 'Others', 'defended', 'Mr', 'Trump', 'saying', 'aside', 'Nambia', 'blunder', 'made', 'valid', 'points', 'Iranian', 'interpreter', 'defends', 'Trump', 'speech', 'omissionsInterpreter', 'defends', 'Trump', 'speech', 'omissionsNima', 'Chitsaz', 'misinterpreted', 'parts', 'Trump', 'UN', 'speech', 'said', 'untrue', 'Iran', 'BBC', 'News', 'Iranian', 'interpreter', 'defends', 'Trump', 'speech', 'omissions']\n"
     ]
    }
   ],
   "source": [
    "print(trumpTokens[1010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "def get_bigrams(tokens_list):\n",
    "    values = []\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    for i in range(len(tokens_list)):\n",
    "        finder = BigramCollocationFinder.from_words(tokens_list[i])\n",
    "        bigrams = list(finder.ngram_fd.items())\n",
    "        bigrams.sort(key=lambda item: item[-1], reverse=True)\n",
    "        values.append(bigrams)\n",
    "    return values\n",
    "\n",
    "trump_bigrams = get_bigrams(trumpTokens)\n",
    "cc_bigrams = get_bigrams(ccTokens)\n",
    "# trigram_finder = TrigramCollocationFinder.from_words(trumpTokens[1010])\n",
    "# trump_trigrams = list(trigram_finder.ngram_fd.items())\n",
    "# trump_trigrams.sort(key=lambda item: item[-1], reverse=True)\n",
    "# trump_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(tokens_list):\n",
    "    values = []\n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "    for i in range(len(tokens_list)):\n",
    "        finder = TrigramCollocationFinder.from_words(tokens_list[i])\n",
    "        trigrams = list(finder.ngram_fd.items())\n",
    "        trigrams.sort(key=lambda item: item[-1], reverse=True)\n",
    "        values.append(trigrams)\n",
    "    return values\n",
    "\n",
    "trump_trigrams = get_trigrams(trumpTokens)\n",
    "cc_trigrams = get_trigrams(ccTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def get_pos_tags(tokens_list):\n",
    "    values = []\n",
    "    for i in range(len(tokens_list)):\n",
    "        current_list = tokens_list[i]\n",
    "        values.append(nltk.pos_tag(current_list))\n",
    "    return values\n",
    "\n",
    "cc_pos_tags = get_pos_tags(ccTokens)\n",
    "trump_pos_tags = get_pos_tags(trumpTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_processed_df = pd.DataFrame({\n",
    "    'sentences': trumpMatrix, \n",
    "    'tokenized_words': trumpTokens,\n",
    "    'bigrams': trump_bigrams,\n",
    "    'trigrams': trump_trigrams,\n",
    "    'pos_tags': trump_pos_tags\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[((Post, Office), 3), ((Mr, Trump), 3), ((atta...</td>\n",
       "      <td>[(President, NNP), (Donald, NNP), (Trump, NNP)...</td>\n",
       "      <td>[President Donald Trump has stepped up his att...</td>\n",
       "      <td>[President, Donald, Trump, stepped, attacks, A...</td>\n",
       "      <td>[((US, Postal, Service), 2), ((US, Post, Offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[((social, media), 4), ((visa, applicants), 4)...</td>\n",
       "      <td>[(The, DT), (Trump, NNP), (administration, NN)...</td>\n",
       "      <td>[The Trump administration has said it wants to...</td>\n",
       "      <td>[The, Trump, administration, said, wants, star...</td>\n",
       "      <td>[((New, York, Times), 2), ((Trump, 'in, crude)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[((North, Korea), 12), ((Korea, talks), 5), ((...</td>\n",
       "      <td>[(Never, RB), (US, NNP), (president, NN), (pre...</td>\n",
       "      <td>[Never before has a US president prepared for ...</td>\n",
       "      <td>[Never, US, president, prepared, summit, impor...</td>\n",
       "      <td>[((Who, going, represent), 3), ((going, repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[((The, US), 1), ((US, president), 1), ((presi...</td>\n",
       "      <td>[(The, DT), (US, NNP), (president, NN), (says,...</td>\n",
       "      <td>[The US president says protecting the Korean b...</td>\n",
       "      <td>[The, US, president, says, protecting, Korean,...</td>\n",
       "      <td>[((The, US, president), 1), ((US, president, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[((Mr, Trump), 4), ((Trump, International), 3)...</td>\n",
       "      <td>[(Donald, NNP), (Trump, NNP), (attempt, NN), (...</td>\n",
       "      <td>[Donald Trump's attempt to dismiss a lawsuit a...</td>\n",
       "      <td>[Donald, Trump, attempt, dismiss, lawsuit, all...</td>\n",
       "      <td>[((Trump, International, Hotel), 3), ((Lawyers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             bigrams  \\\n",
       "0  [((Post, Office), 3), ((Mr, Trump), 3), ((atta...   \n",
       "1  [((social, media), 4), ((visa, applicants), 4)...   \n",
       "2  [((North, Korea), 12), ((Korea, talks), 5), ((...   \n",
       "3  [((The, US), 1), ((US, president), 1), ((presi...   \n",
       "4  [((Mr, Trump), 4), ((Trump, International), 3)...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(President, NNP), (Donald, NNP), (Trump, NNP)...   \n",
       "1  [(The, DT), (Trump, NNP), (administration, NN)...   \n",
       "2  [(Never, RB), (US, NNP), (president, NN), (pre...   \n",
       "3  [(The, DT), (US, NNP), (president, NN), (says,...   \n",
       "4  [(Donald, NNP), (Trump, NNP), (attempt, NN), (...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [President Donald Trump has stepped up his att...   \n",
       "1  [The Trump administration has said it wants to...   \n",
       "2  [Never before has a US president prepared for ...   \n",
       "3  [The US president says protecting the Korean b...   \n",
       "4  [Donald Trump's attempt to dismiss a lawsuit a...   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [President, Donald, Trump, stepped, attacks, A...   \n",
       "1  [The, Trump, administration, said, wants, star...   \n",
       "2  [Never, US, president, prepared, summit, impor...   \n",
       "3  [The, US, president, says, protecting, Korean,...   \n",
       "4  [Donald, Trump, attempt, dismiss, lawsuit, all...   \n",
       "\n",
       "                                            trigrams  \n",
       "0  [((US, Postal, Service), 2), ((US, Post, Offic...  \n",
       "1  [((New, York, Times), 2), ((Trump, 'in, crude)...  \n",
       "2  [((Who, going, represent), 3), ((going, repres...  \n",
       "3  [((The, US, president), 1), ((US, president, s...  \n",
       "4  [((Trump, International, Hotel), 3), ((Lawyers...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_processed_df = pd.DataFrame({\n",
    "    'sentences': ccMatrix, \n",
    "    'tokenized_words': ccTokens,\n",
    "    'bigrams': cc_bigrams,\n",
    "    'trigrams': cc_trigrams,\n",
    "    'pos_tags': cc_pos_tags\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>[((away, coal), 4), ((climate, change), 4), ((...</td>\n",
       "      <td>[(The, DT), (UK, NNP), (Canada, NNP), (launche...</td>\n",
       "      <td>[The UK and Canada have launched a global alli...</td>\n",
       "      <td>[The, UK, Canada, launched, global, alliance, ...</td>\n",
       "      <td>[((time, next, major), 2), ((next, major, UN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>[((food, waste), 9), ((surplus, food), 5), ((B...</td>\n",
       "      <td>[(A, DT), (rising, VBG), (number, NN), (firms,...</td>\n",
       "      <td>[A rising number of firms are finding creative...</td>\n",
       "      <td>[A, rising, number, firms, finding, creative, ...</td>\n",
       "      <td>[((issue, food, waste), 2), ((A, rising, numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>[((Meeting, carbon-reduction), 1), ((carbon-re...</td>\n",
       "      <td>[(Meeting, VBG), (carbon-reduction, JJ), (targ...</td>\n",
       "      <td>[Meeting carbon-reduction targets will be more...</td>\n",
       "      <td>[Meeting, carbon-reduction, targets, challengi...</td>\n",
       "      <td>[((Meeting, carbon-reduction, targets), 1), ((...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>[((More, hurricanes), 1), ((hurricanes, Rising...</td>\n",
       "      <td>[(More, JJR), (hurricanes, NNS), (Rising, VBG)...</td>\n",
       "      <td>[More hurricanes?, Rising temperatures?, BBC m...</td>\n",
       "      <td>[More, hurricanes, Rising, temperatures, BBC, ...</td>\n",
       "      <td>[((More, hurricanes, Rising), 1), ((hurricanes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>[((UK, US), 1), ((US, scientists), 1), ((scien...</td>\n",
       "      <td>[(UK, NNP), (US, NNP), (scientists, NNS), (lea...</td>\n",
       "      <td>[UK and US scientists will lead a five-year ef...</td>\n",
       "      <td>[UK, US, scientists, lead, five-year, effort, ...</td>\n",
       "      <td>[((UK, US, scientists), 1), ((US, scientists, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               bigrams  \\\n",
       "657  [((away, coal), 4), ((climate, change), 4), ((...   \n",
       "658  [((food, waste), 9), ((surplus, food), 5), ((B...   \n",
       "659  [((Meeting, carbon-reduction), 1), ((carbon-re...   \n",
       "660  [((More, hurricanes), 1), ((hurricanes, Rising...   \n",
       "661  [((UK, US), 1), ((US, scientists), 1), ((scien...   \n",
       "\n",
       "                                              pos_tags  \\\n",
       "657  [(The, DT), (UK, NNP), (Canada, NNP), (launche...   \n",
       "658  [(A, DT), (rising, VBG), (number, NN), (firms,...   \n",
       "659  [(Meeting, VBG), (carbon-reduction, JJ), (targ...   \n",
       "660  [(More, JJR), (hurricanes, NNS), (Rising, VBG)...   \n",
       "661  [(UK, NNP), (US, NNP), (scientists, NNS), (lea...   \n",
       "\n",
       "                                             sentences  \\\n",
       "657  [The UK and Canada have launched a global alli...   \n",
       "658  [A rising number of firms are finding creative...   \n",
       "659  [Meeting carbon-reduction targets will be more...   \n",
       "660  [More hurricanes?, Rising temperatures?, BBC m...   \n",
       "661  [UK and US scientists will lead a five-year ef...   \n",
       "\n",
       "                                       tokenized_words  \\\n",
       "657  [The, UK, Canada, launched, global, alliance, ...   \n",
       "658  [A, rising, number, firms, finding, creative, ...   \n",
       "659  [Meeting, carbon-reduction, targets, challengi...   \n",
       "660  [More, hurricanes, Rising, temperatures, BBC, ...   \n",
       "661  [UK, US, scientists, lead, five-year, effort, ...   \n",
       "\n",
       "                                              trigrams  \n",
       "657  [((time, next, major), 2), ((next, major, UN),...  \n",
       "658  [((issue, food, waste), 2), ((A, rising, numbe...  \n",
       "659  [((Meeting, carbon-reduction, targets), 1), ((...  \n",
       "660  [((More, hurricanes, Rising), 1), ((hurricanes...  \n",
       "661  [((UK, US, scientists), 1), ((US, scientists, ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_processed_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3245    [US President-elect Donald Trump says there mu...\n",
       "3246    [Ford's decision to cancel a $1.6bn investment...\n",
       "3247    [A leading US Republican says she fears for th...\n",
       "3248    [2016 has been one dubbed for the history book...\n",
       "3249    [Thirty-five Russian diplomats expelled from t...\n",
       "Name: sentences, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_processed_df['sentences'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-Based Autosummarizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import FreqDist\n",
    "\n",
    "# freq = FreqDist(trump_processd_df.tokenized_words.iloc[0])\n",
    "# freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Currently, we still have two large datasets of BBC News articles; one tagged with the topic 'Donald Trump' and the other with 'Climate Change'. All articles tagged with these topics are mostly about Donald Trump or Climage Change, they will also touch on different areas also, and have different newsworthy angles and focuses. \n",
    "\n",
    "I decided that it would be good to investigate where there are any specific angles that are covered regularly. I will refer to these as 'themes' hereafter. It would interesting to see if there are any recurring themes in articles about these topics. I am asking: is something very specific written about very often under these given topics? If so, this _might_ suggest bias. \n",
    "\n",
    "Each dataset could be divided into groups, or themes, based on some measure of similarity that might be present. There is likely to be a difference in the common attributes of the Donald Trump articles compared with the Climate Change articles. All articles that represent a particular 'theme' within each dataset should be clustered into one group so they might then be compared and analysed for evidence impartiality. \n",
    "\n",
    "This is a clustering problem. Clustering is defined as grouping objects which are similar to each other and dissimilar to the objects in the other groupings (Bramer 2007). The objective in clusering is to maximise intracluster similarity and minimize intercluster similarity (Kolalapudi 2016).\n",
    "\n",
    "## Feature Extraction\n",
    "\n",
    "The first step is representing text data using numeric attributes called features and the process of extracting them from the text is called feature extraction (Kolalapudi 2016). There are various methods by which feature extraction can be performed, but the methods I shall be concerned with here are called 'Term Frequency' and 'TF-IDF'. \n",
    "\n",
    "### Term Frequency Representation using the 'Bag of Words' model.\n",
    "Steps to performing Term Frequency\n",
    "1. Create a list of all possible words that could appear any article. The list is of length `n`, where `n` is the number of words in the text. \n",
    "2. A tuple is created for each article where each word in the list of all words is represented by the number of times it occurred in the text. \n",
    "For example,\n",
    "if all the possible words are `(these, are, all, the, possible, words, that, could, appear)` then the sentence `all these words are possible` would produce the tuple `(1, 1, 1, 0, 1, 1, 0, 0, 0)` which is called the Term Frequency Representation.\n",
    "\n",
    "It is important to note all information regarding the order of the words in the text is lost, this is why this model is called 'Bag of Words' because it is likened to putting all the words in a bag (Kolalapudi 2016). This is suitable for our initial experiment in discovering themes because we will be clustering based on the words themselves not the order they appeared in. \n",
    "\n",
    "### Term Freqency - Inverse Document Frequency\n",
    "Term Freqency - Inverse Document Frequency (TF-IDF) enhances the Term Frequency Representation by recognising that some words are more important than others (Kolalapudi 2016). TF-IDF works on the widely held supposition that a seldomly used word found in a sentence generally has more of an impact on the meaning of that sentence than other words (Kolalapudi 2016). By taking this into account, I would like to a way of 'weighting' each word in my features to take into account its rarity. TF-IDF is a commonly used way of doing this. \n",
    "\n",
    "The method of finding the TF-IDF is to take the corpus, which is the entire dataset of articles, and the tuples of word frequencies, then weight each word's frequency by the inverse of the number of articles the word is present in (Kolalapudi 2016). The term frequency is multiplied by the inverse of the document frequency which is why this method is called 'Term Freqency - Inverse Document Frequency' (Kolalapudi 2016). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Corpus\n",
    "\n",
    "I start the process of feature extraction by getting the corpuses of each dataset. I am using the `tokenized_words` to do this because it is my cleanest respresentation of the data so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df, column):\n",
    "    corpus = []\n",
    "    for i in range (len(df)):\n",
    "        text = df[column].iloc[i]\n",
    "        corpus.append(' '.join(text))\n",
    "    return corpus\n",
    "\n",
    "cc_corpus = get_corpus(cc_processed_df, 'tokenized_words')\n",
    "trump_corpus = get_corpus(trump_processed_df, 'tokenized_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the TF-IDF\n",
    "\n",
    "I am using a popular Data Science library, specifically for machine learning problems called SciKit Learn. I am using  built-in methods for creating the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3250x36560 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 697087 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tfid = TfidfVectorizer().fit_transform(trump_corpus)\n",
    "trump_tfid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<662x12800 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 146686 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_tfid = TfidfVectorizer().fit_transform(cc_corpus)\n",
    "cc_tfid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "K-means clustering is described by Bramner (2007) as an 'exclusive clustering algorithm' because it assigns each object to only one of a set of clusters. This method was chosen because I am looking for distinct overarching themes that may be present within the news articles, I am not looking to put them into a hierarchy for example.\n",
    "\n",
    "Steps to performing k-means clustering (Bramner 2007).\n",
    "1. Decide on a value of `k` which is the number of clusters that should be found. \n",
    "2. Select the `k` objects in an arbitary fashion, these should be the inital set of `k` centroids. \n",
    "3. Each object is assigned one by one to the cluster which has the closet centroid. \n",
    "4. Now, it is very likely that the original centroids will no longer be the true centroids of each clusters, therefore a recalculation must be made and the process is repeated. \n",
    "5. Repeat steps 3. and 4. until the centroids do not change and convergence has been reached. \n",
    "\n",
    "What SciKit Learn referrs to as the `inertia` is what is usually referred to as the 'objective function': the sum of the squared distances of objects to their closest cluster centre (SciKit Learn 2017; Bramner 2007). The objective function should decrease with each iteration until the best clustering is found. \n",
    "\n",
    "Below the number of clusters is set to 3 and the maximum number of iterations is set to 99. I have chosen 3 from running this experiment several times and I find it to be a good number. Thankfully, the number of iterations before convergence has never gotten close to 99 in all the times I have run this experiment, but it is important to set the maximum number of iterations anyway (Bramner 2007). This is because although it can be proved that k-means clustering will always terminate, it does not always find the best set of clusters, corresponding the minimising the value of the objective function (Bramner 2007). \n",
    "\n",
    "The initial selection of centroids can too significantly affect the result (Bramner 2007). To overcome this, the algorithm can be run several times for a given value of `k`, each time with a different choice of inital `k` centroids (Bramner 2007). The set of clusters with the smallest value of the objective function is then chosen (Bramner 2007). The `n_init` value set is the 'number of time the k-means algorithm will be run with different centroid seeds' (SciKit Learn 2017). The final results from SciKit Learn are the best output of the `n_init` consecutive runs in terms of intertia (SciKit Learn 2017). From running this experiment several times and tweaking the numbers I have found that 4 is a good number of iterations to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=99,\n",
       "    n_clusters=3, n_init=4, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cc_km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 99, n_init = 4, verbose = False )\n",
    "\n",
    "cc_km.fit(cc_tfid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a simple method to print some of the findings because the method's output, when set to verbose, is too verbose.\n",
    "\n",
    "The labels show that there are 3 clusters (but we knew that because we set it) and it also shows the number of articles which have been assigned to each cluster in the second array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective funtion: 602.7085761970471\n",
      " coordinates of cluster centres:\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00093856 0.00473292 0.         ... 0.         0.         0.        ]\n",
      " [0.00177682 0.00908636 0.00149422 ... 0.00017678 0.00034052 0.00018628]]\n",
      "labels: \n",
      "(array([0, 1, 2], dtype=int32), array([ 52, 174, 436]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_output(km): \n",
    "    string = 'objective funtion: {0}\\n coordinates of cluster centres:\\n {1}'.format(km.inertia_, km.cluster_centers_)\n",
    "    labels = 'labels: \\n{}'.format(np.unique(km.labels_ , return_counts=True))\n",
    "    print(string)\n",
    "    print(labels)\n",
    "    \n",
    "print_output(cc_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=99,\n",
       "    n_clusters=3, n_init=4, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 99, n_init = 4, verbose = False )\n",
    "trump_km.fit(trump_tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective funtion: 3016.114723836685\n",
      " coordinates of cluster centres:\n",
      " [[1.67184372e-03 7.68397213e-03 1.38564640e-05 ... 9.29486553e-05\n",
      "  2.56751307e-05 2.56751307e-05]\n",
      " [8.10959849e-04 2.12962453e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.95050159e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "labels: \n",
      "(array([0, 1, 2], dtype=int32), array([2684,  401,  165]))\n"
     ]
    }
   ],
   "source": [
    "print_output(trump_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the Themes\n",
    "\n",
    "The method below creates a dictionary for each corpus whereby the keys are the cluster numbers and the values are the aggregated text across all the articles that are within that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_from_corpus(km, corpus):\n",
    "    text = {}\n",
    "    for i, cluster in enumerate(km.labels_):\n",
    "        current = corpus[i]\n",
    "        if cluster not in text.keys():\n",
    "            text[cluster] = current\n",
    "        else:\n",
    "            text[cluster] += current\n",
    "    return text\n",
    "\n",
    "cc_cluster_dict = dictionary_from_corpus(cc_km, cc_corpus)\n",
    "trump_cluster_dict = dictionary_from_corpus(trump_km, trump_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to find the most frequent words within that cluster. \n",
    "\n",
    "Through running and tweaking this experiment, I found it necessary add 'bbc' to the set of `stop_words` because it was appearing too often and is not helpful in identifying themes in BBC News articles. By the end of this method, the words have been tokenized, re-joined to find the TF-IDF and then tokenized again within this method. It is likely because of this that something odd was happening: 'n't' got treated as a word and was appearing often; therefore I also added this to the `stop_words`. \n",
    "\n",
    "`freq = FreqDist(word_sent)` calculates the frequency distribution of the words and `keywords[cluster] = nlargest(99, freq, key=freq.get)` finds the top 100 words in that cluster (set to 99 because Python is zero-based). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from heapq import nlargest\n",
    "\n",
    "stop_words.add('bbc')\n",
    "stop_words.add(\"n't\")\n",
    "\n",
    "def get_freq_dist(dict):\n",
    "    keywords = {}\n",
    "    counts = {}\n",
    "    for cluster in range(len(dict)):\n",
    "        word_sent = word_tokenize(dict[cluster].lower())\n",
    "        word_sent = [word for word in word_sent if word not in stop_words]\n",
    "        freq = FreqDist(word_sent)\n",
    "        keywords[cluster] = nlargest(99, freq, key=freq.get)\n",
    "        counts[cluster] = freq\n",
    "    return (keywords, counts)\n",
    "\n",
    "cc_word_counts = get_freq_dist(cc_cluster_dict)\n",
    "trump_word_counts = get_freq_dist(trump_cluster_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_keys(data, num_of_unique_keys=10):\n",
    "    keywords = data[0]\n",
    "    counts = data[1]\n",
    "    unique_keys = {}\n",
    "    length = len(counts)\n",
    "    for cluster in range(length):\n",
    "        other_clusters = list(set(range(length))-set([cluster]))\n",
    "        keys_other_clusters = set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "        unique=set(keywords[cluster])-keys_other_clusters\n",
    "        unique_keys[cluster]=nlargest(num_of_unique_keys,unique,key=counts[cluster].get)\n",
    "    return unique_keys\n",
    "\n",
    "cc_12_unique_keys = get_unique_keys(cc_word_counts, 12)\n",
    "trump_12_unique_keys = get_unique_keys(trump_word_counts, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['america',\n",
       "  'order',\n",
       "  'ban',\n",
       "  'american',\n",
       "  'like',\n",
       "  'countries',\n",
       "  'back',\n",
       "  'climate',\n",
       "  'since',\n",
       "  'office',\n",
       "  'tax',\n",
       "  'immigration'],\n",
       " 1: ['russian',\n",
       "  'comey',\n",
       "  'fbi',\n",
       "  'investigation',\n",
       "  'intelligence',\n",
       "  'flynn',\n",
       "  'director',\n",
       "  'clinton',\n",
       "  'mueller',\n",
       "  'putin',\n",
       "  'team',\n",
       "  'committee'],\n",
       " 2: ['north',\n",
       "  'korea',\n",
       "  'nuclear',\n",
       "  'iran',\n",
       "  'korean',\n",
       "  'kim',\n",
       "  'missile',\n",
       "  'south',\n",
       "  'pyongyang',\n",
       "  'un',\n",
       "  'weapons',\n",
       "  'jong-un']}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_12_unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['women',\n",
       "  'coastal',\n",
       "  'flooding',\n",
       "  'erosion',\n",
       "  'risk',\n",
       "  'rising',\n",
       "  'affected',\n",
       "  'mercury',\n",
       "  'sites',\n",
       "  'acidification',\n",
       "  'events',\n",
       "  'food'],\n",
       " 1: ['trump',\n",
       "  'president',\n",
       "  'agreement',\n",
       "  'deal',\n",
       "  'countries',\n",
       "  'accord',\n",
       "  'donald',\n",
       "  'china',\n",
       "  'decision',\n",
       "  'administration',\n",
       "  'house',\n",
       "  'white'],\n",
       " 2: ['ice',\n",
       "  'uk',\n",
       "  'around',\n",
       "  'air',\n",
       "  'two',\n",
       "  'dr',\n",
       "  'use',\n",
       "  'temperature',\n",
       "  'data',\n",
       "  'prof',\n",
       "  'heat',\n",
       "  'need']}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_12_unique_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "Much of what has been carried out here is not very efficient and would not be suitable for big data. During the course of finding the themes, the text was initally tokenized as part of the data cleaning, re-joined to find the TF-IDF which takes string inputs, and then tokenized again to find the top 100 words. This is what likely lead to 'n't' being treated as a word. Clearly, this isn't efficient and it also lead to bizarre behaviour, which thankfully was caught. In addition to this, nearly all methods involved looping over the large datasets. Running all the methods involves doing so multiple times. A much more efficient workflow must be devised with the complexity of each algorithm carefully considered and could ultilize features of NumPy to help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Bramer, M. (2007) _Principles of Data Mining_. UK: Springer Science<br />\n",
    "Kolalapudi, S. (2016) _Understanding the Clustering Workflow_ [video]. https://app.pluralsight.com/library/courses/python-natural-language-processing/table-of-contents. Accessed 29<sup>th</sup> May 2018. <br />\n",
    "SciKit Learn (2017) _sklearn.cluster.KMeans_. SciKit Learn Documentation. http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Accessed 15<sup>th</sup> June 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
