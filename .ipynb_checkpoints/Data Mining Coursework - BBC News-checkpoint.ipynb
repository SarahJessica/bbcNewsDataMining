{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining on BBC News Topics\n",
    "\n",
    "This project is coursework for the Data Mining module, which is part of the MSc Software Engineering and Internet Architecture programme at The University of Bradford.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The BBC states that \"impartiality lies at the heart of public service and is the core of [our] commitment to... audiences\" (BBC 2010). The first of the BBC's six values makes it imperative: \"Trust is the foundation of the BBC; we are independent, impartial and honest\" (BBC 2018). The BBC is committed to provide impartial content, not only in terms of ethics, but it is also stipulated in the Royal Charter, which is \"is the constitutional basis for the BBC... [that] sets out the public purposes of the BBC [and] guarantees its independence\" (BBC Trust 2017). The Royal Charter mandates that the BBC must treat particularly controversial subjects in the news with due impartiality (BBC 2010). The BBC's Editorial Guidelines go futher than that, and strive for \"due impartiality\" across all news topics (BBC 2010).\n",
    "\n",
    "The BBC has strong reasons for monitoring the impartiality, or lack thereof, in its news output, especially as it has often been criticized for not living up to this core obligation. One very public criticism of BBC News impartiality standards was made on 16<sup>th</sup> February 2017 when Donald Trump, after asking a BBC reporter where he was from, disparagingly called the BBC \"another beauty\" (BBC News 2017a). When the reporter quoted part of the BBC values statement in response, Trump dismissed this counter with \"yeah, sure... Just like CNN right?\" (BBC News 2017a). Later that same month the White House banned BBC News from attending an imformal press briefing, incidentally CNN was also banned along with the New York Times (BBC News 2017b). Although he has never said this directly (to my knowledge) it strongly implies that US President Donald Trump does not believe that the BBC is impartial towards him. Therefore I decided that Donald Trump would be an interesting topic for investigation. \n",
    "\n",
    "BBC News is open about the fact that it finds impartiality about Climate Change difficult (Warburton et al. 2018). The BBC now accepts that disscenting voices on Climate Change must have the scientific basic to support them, otherwise the subject can suffer from 'false balance' or 'false equivalence' where pitting a scientist against a skeptic gives the audience a false impression of the argument (Warburton et al. 2018). The requirement of giving \"due weight\" has now been added to the editorial guidelines in order to avoid giving the an impression that skeptics have an equal weight of opinion against scientists from the global warming community (Warburton et al. 2018; BBC 2010). The BBC is incombant on presenters being knowledagble enough to challenge those being interviewed, and as a result, BBC News journalists wishing to write about Climate Change are required to attend a specialist training course (Warburton et al. 2018). Since the BBC is going to great pains to show due impartiality in regard to Climate Change, I decided to pick this as my second topic. \n",
    "\n",
    "The subject of determining a measure of impartiality of news stories is a massive problem space and is out of the scope for this coursework. The aim of this coursework is to conduct some preliminary investigations that might give guidance regarding the best and most futile approaches to take in futher research.\n",
    "\n",
    "### Introduction to the Data\n",
    "\n",
    "BBC News, along with other BBC online content, is tagged with metadata managed by an internal team called Linked Data Platform (LDP) and these tags are known as LDP tags. Some LDP tags have special relevance and are called a 'topic tag'. When a news story is tagged with a topic tag, it suggests that that story predominantly belongs to a particular topic, such as a place or person. I am interested in the topics 'Donald Trump' and 'Climate Change'.  \n",
    "\n",
    "The data is all publicly available on the various BBC News websites and could be obtained through the difficult task of crawling BBC News online, finding news stories tagged with 'Climate Change' or 'Donald Trump', then selecting, scraping and storing the relevant text. As a Software Engineer at the BBC, I was able to ask a colleague working in LDP for a favour. He obtained the all the BBC News articles which were tagged with 'Climate Change' or 'Donald Trump'  and were last published between January 2017 and April 2018 for me. He transferred them to me as two large JSON documents, one for each topic. \n",
    "\n",
    "I am using Pandas, which is a Python-based Data Analytics Library for most of this coursework (Pandas 2018). I will be utilizing Pandas dataframes which are a convenient way to read in and work with JSON data and also visualize them in a table. I start by reading in and looking at the data that I have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "allCc = pd.read_json('climate_change_data_subset.json')\n",
    "allTrump = pd.read_json('trump_data_subset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetId</th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>language</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38391034</td>\n",
       "      <td>/news/business-38391034</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>India's double first in climate battle</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2017-01-08T14:55:00+00:00</td>\n",
       "      <td>India opens two world-leading clean energy pro...</td>\n",
       "      <td>BBC News - India's double first in climate battle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38527710</td>\n",
       "      <td>/ukrainian/news-38527710</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>В Україну з півдня насуваються снігопади</td>\n",
       "      <td>uk</td>\n",
       "      <td>2017-01-06T06:31:56+00:00</td>\n",
       "      <td>Через циклон, який насувається з Чорного моря,...</td>\n",
       "      <td>BBC Україна - В Україну з півдня насуваються с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38600431</td>\n",
       "      <td>/news/uk-wales-38600431</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>£3m to protect Welsh and Irish coast from clim...</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2017-01-13T12:48:42+00:00</td>\n",
       "      <td>Coastal tourist sites most affected by climate...</td>\n",
       "      <td>BBC News - £3m to protect Welsh and Irish coas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38745937</td>\n",
       "      <td>/news/science-environment-38745937</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Defining a true 'pre-industrial' climate period</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2017-01-25T23:11:25+00:00</td>\n",
       "      <td>Scientists suggest pushing back a baseline fro...</td>\n",
       "      <td>BBC News - Defining a true 'pre-industrial' cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39053678</td>\n",
       "      <td>/news/science-environment-39053678</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Most wood energy schemes are a 'disaster' for ...</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2017-04-23T14:02:03+00:00</td>\n",
       "      <td>A new report says that using wood pellets to g...</td>\n",
       "      <td>BBC News - Most wood energy schemes are a 'dis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    assetId                            assetUri  \\\n",
       "0  38391034             /news/business-38391034   \n",
       "1  38527710            /ukrainian/news-38527710   \n",
       "2  38600431             /news/uk-wales-38600431   \n",
       "3  38745937  /news/science-environment-38745937   \n",
       "4  39053678  /news/science-environment-39053678   \n",
       "\n",
       "                                                body  \\\n",
       "0  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "1  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline language  \\\n",
       "0             India's double first in climate battle    en-gb   \n",
       "1           В Україну з півдня насуваються снігопади       uk   \n",
       "2  £3m to protect Welsh and Irish coast from clim...    en-gb   \n",
       "3    Defining a true 'pre-industrial' climate period    en-gb   \n",
       "4  Most wood energy schemes are a 'disaster' for ...    en-gb   \n",
       "\n",
       "               lastPublished  \\\n",
       "0  2017-01-08T14:55:00+00:00   \n",
       "1  2017-01-06T06:31:56+00:00   \n",
       "2  2017-01-13T12:48:42+00:00   \n",
       "3  2017-01-25T23:11:25+00:00   \n",
       "4  2017-04-23T14:02:03+00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "0  India opens two world-leading clean energy pro...   \n",
       "1  Через циклон, який насувається з Чорного моря,...   \n",
       "2  Coastal tourist sites most affected by climate...   \n",
       "3  Scientists suggest pushing back a baseline fro...   \n",
       "4  A new report says that using wood pellets to g...   \n",
       "\n",
       "                                               title  \n",
       "0  BBC News - India's double first in climate battle  \n",
       "1  BBC Україна - В Україну з півдня насуваються с...  \n",
       "2  BBC News - £3m to protect Welsh and Irish coas...  \n",
       "3  BBC News - Defining a true 'pre-industrial' cl...  \n",
       "4  BBC News - Most wood energy schemes are a 'dis...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allCc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetId</th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>language</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43597394</td>\n",
       "      <td>/portuguese/internacional-43597394</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>A pouco conhecida história de como os EUA leva...</td>\n",
       "      <td>pt-BR</td>\n",
       "      <td>2018-04-01T17:43:20+00:00</td>\n",
       "      <td>O governo americano foi o primeiro a violar o ...</td>\n",
       "      <td>BBC Brasil - A pouco conhecida história de com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43607394</td>\n",
       "      <td>/portuguese/internacional-43607394</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>O ativista que criou uma identidade falsa para...</td>\n",
       "      <td>pt-BR</td>\n",
       "      <td>2018-04-02T07:34:34+00:00</td>\n",
       "      <td>Um ativista antirracismo passou um ano infiltr...</td>\n",
       "      <td>BBC Brasil - O ativista que criou uma identida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43606088</td>\n",
       "      <td>/news/world-us-canada-43606088</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Donald Trump steps up attacks on Amazon</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2018-03-31T21:23:32+00:00</td>\n",
       "      <td>The president accuses the online retail giant ...</td>\n",
       "      <td>BBC News - Donald Trump steps up attacks on Am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43601557</td>\n",
       "      <td>/news/world-us-canada-43601557</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>US may tie social media to visa applications</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2018-03-31T12:55:24+00:00</td>\n",
       "      <td>A state department proposal could require visa...</td>\n",
       "      <td>BBC News - US may tie social media to visa app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43578462</td>\n",
       "      <td>/news/world-us-canada-43578462</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Is Trump ready for talks with North Korea?</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>2018-03-30T21:20:28+00:00</td>\n",
       "      <td>What preparations is the White House making fo...</td>\n",
       "      <td>BBC News - Is Trump ready for talks with North...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    assetId                            assetUri  \\\n",
       "0  43597394  /portuguese/internacional-43597394   \n",
       "1  43607394  /portuguese/internacional-43607394   \n",
       "2  43606088      /news/world-us-canada-43606088   \n",
       "3  43601557      /news/world-us-canada-43601557   \n",
       "4  43578462      /news/world-us-canada-43578462   \n",
       "\n",
       "                                                body  \\\n",
       "0  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "1  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline language  \\\n",
       "0  A pouco conhecida história de como os EUA leva...    pt-BR   \n",
       "1  O ativista que criou uma identidade falsa para...    pt-BR   \n",
       "2            Donald Trump steps up attacks on Amazon    en-gb   \n",
       "3       US may tie social media to visa applications    en-gb   \n",
       "4         Is Trump ready for talks with North Korea?    en-gb   \n",
       "\n",
       "               lastPublished  \\\n",
       "0  2018-04-01T17:43:20+00:00   \n",
       "1  2018-04-02T07:34:34+00:00   \n",
       "2  2018-03-31T21:23:32+00:00   \n",
       "3  2018-03-31T12:55:24+00:00   \n",
       "4  2018-03-30T21:20:28+00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "0  O governo americano foi o primeiro a violar o ...   \n",
       "1  Um ativista antirracismo passou um ano infiltr...   \n",
       "2  The president accuses the online retail giant ...   \n",
       "3  A state department proposal could require visa...   \n",
       "4  What preparations is the White House making fo...   \n",
       "\n",
       "                                               title  \n",
       "0  BBC Brasil - A pouco conhecida história de com...  \n",
       "1  BBC Brasil - O ativista que criou uma identida...  \n",
       "2  BBC News - Donald Trump steps up attacks on Am...  \n",
       "3  BBC News - US may tie social media to visa app...  \n",
       "4  BBC News - Is Trump ready for talks with North...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Immediately, by looking at the `head()` which shows me the first five lines in the dataframe, I notice two things. \n",
    "1. That the `lastPublished` dates run in the opposite orders. \n",
    "2. I have BBC News articles from across all the BBC News websites regardless of language. \n",
    "\n",
    "I am only interested in UK Domestic news in English so I address this first. I know from previously working on BBC News that these are all available in the `assetURI` path beginning with `/news`. I start by disregarding those that are not UK Domestic English language news.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL LANGUAGES \n",
      " Climate Change: 1808\n",
      " Trump: 5135\n"
     ]
    }
   ],
   "source": [
    "print('ALL LANGUAGES \\n Climate Change: {0}\\n Trump: {1}'.format(len(allCc), len(allTrump)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = allCc[allCc.assetUri.str.startswith('/news')]\n",
    "trump = allTrump[allTrump.assetUri.str.startswith('/news')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH UK DOMESTIC \n",
      " Climate Change: 664\n",
      " Trump: 3254\n"
     ]
    }
   ],
   "source": [
    "print('ENGLISH UK DOMESTIC \\n Climate Change: {0}\\n Trump: {1}'.format(len(cc), len(trump)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can immediately see that I now have far fewer articles for each topic. We can conduct a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(trump['language'] != 'en-gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cc['language'] != 'en-gb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will find out more information about the data I have remaining by printing the `.info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3254 entries, 2 to 5133\n",
      "Data columns (total 8 columns):\n",
      "assetId          3254 non-null int64\n",
      "assetUri         3254 non-null object\n",
      "body             3250 non-null object\n",
      "headline         3254 non-null object\n",
      "language         3254 non-null object\n",
      "lastPublished    3254 non-null object\n",
      "summary          3254 non-null object\n",
      "title            3254 non-null object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 228.8+ KB\n"
     ]
    }
   ],
   "source": [
    "trump.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 664 entries, 0 to 1805\n",
      "Data columns (total 8 columns):\n",
      "assetId          664 non-null int64\n",
      "assetUri         664 non-null object\n",
      "body             662 non-null object\n",
      "headline         664 non-null object\n",
      "language         664 non-null object\n",
      "lastPublished    664 non-null object\n",
      "summary          664 non-null object\n",
      "title            664 non-null object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 46.7+ KB\n"
     ]
    }
   ],
   "source": [
    "cc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that four of the Donald Trump bodies are missing and also two of the Climate Change bodies. It is the main body of the news story that I am interested in, therefore these can be regarded. Keeping them would likely result in strange behaviour or even errors in data experiments later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = cc.dropna(axis=0, how='any')\n",
    "trump = trump.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am also not interested in keeping the assetId or the language (which by now is all `en-gb` anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = cc.drop(columns=['assetId','language'])\n",
    "trump = trump.drop(columns=['assetId','language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body of the article is not in plain text, it is stored in XML. The XML tags need to be removed in order to work with the text. I started by attempting to use open source libraries like Beautiful Soup, however, they are mainly for HTML tags. In addition, BBC News uses its own internal variant of XML called CandyXML and is therefore not entirely standard. I found it necessary to write my own data extraction method which removes the extraneous tags and other strange attributes I found by looking through the data. Since I am working with Pandas dataframes, I extracted the clean text straight into a new column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>cleanBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/news/world-us-canada-43606088</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Donald Trump steps up attacks on Amazon</td>\n",
       "      <td>2018-03-31T21:23:32+00:00</td>\n",
       "      <td>The president accuses the online retail giant ...</td>\n",
       "      <td>BBC News - Donald Trump steps up attacks on Am...</td>\n",
       "      <td>President Donald Trump has stepped up his atta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/news/world-us-canada-43601557</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>US may tie social media to visa applications</td>\n",
       "      <td>2018-03-31T12:55:24+00:00</td>\n",
       "      <td>A state department proposal could require visa...</td>\n",
       "      <td>BBC News - US may tie social media to visa app...</td>\n",
       "      <td>The Trump administration has said it wants to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/news/world-us-canada-43578462</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Is Trump ready for talks with North Korea?</td>\n",
       "      <td>2018-03-30T21:20:28+00:00</td>\n",
       "      <td>What preparations is the White House making fo...</td>\n",
       "      <td>BBC News - Is Trump ready for talks with North...</td>\n",
       "      <td>Never before has a US president prepared for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/news/world-us-canada-43591904</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump: Billions spent protecting other countri...</td>\n",
       "      <td>2018-03-29T19:31:50+00:00</td>\n",
       "      <td>Mr Trump says securing the Korean border comes...</td>\n",
       "      <td>BBC News - Trump: Billions spent protecting ot...</td>\n",
       "      <td>The US president says protecting the Korean bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/news/world-us-canada-43577444</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump loses bid to dismiss hotel lawsuit</td>\n",
       "      <td>2018-03-29T02:13:35+00:00</td>\n",
       "      <td>A judge allows a case to determine if the pres...</td>\n",
       "      <td>BBC News - Trump loses bid to dismiss hotel la...</td>\n",
       "      <td>Donald Trump's attempt to dismiss a lawsuit al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         assetUri  \\\n",
       "2  /news/world-us-canada-43606088   \n",
       "3  /news/world-us-canada-43601557   \n",
       "4  /news/world-us-canada-43578462   \n",
       "6  /news/world-us-canada-43591904   \n",
       "7  /news/world-us-canada-43577444   \n",
       "\n",
       "                                                body  \\\n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "6  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "7  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline  \\\n",
       "2            Donald Trump steps up attacks on Amazon   \n",
       "3       US may tie social media to visa applications   \n",
       "4         Is Trump ready for talks with North Korea?   \n",
       "6  Trump: Billions spent protecting other countri...   \n",
       "7           Trump loses bid to dismiss hotel lawsuit   \n",
       "\n",
       "               lastPublished  \\\n",
       "2  2018-03-31T21:23:32+00:00   \n",
       "3  2018-03-31T12:55:24+00:00   \n",
       "4  2018-03-30T21:20:28+00:00   \n",
       "6  2018-03-29T19:31:50+00:00   \n",
       "7  2018-03-29T02:13:35+00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "2  The president accuses the online retail giant ...   \n",
       "3  A state department proposal could require visa...   \n",
       "4  What preparations is the White House making fo...   \n",
       "6  Mr Trump says securing the Korean border comes...   \n",
       "7  A judge allows a case to determine if the pres...   \n",
       "\n",
       "                                               title  \\\n",
       "2  BBC News - Donald Trump steps up attacks on Am...   \n",
       "3  BBC News - US may tie social media to visa app...   \n",
       "4  BBC News - Is Trump ready for talks with North...   \n",
       "6  BBC News - Trump: Billions spent protecting ot...   \n",
       "7  BBC News - Trump loses bid to dismiss hotel la...   \n",
       "\n",
       "                                           cleanBody  \n",
       "2  President Donald Trump has stepped up his atta...  \n",
       "3  The Trump administration has said it wants to ...  \n",
       "4  Never before has a US president prepared for a...  \n",
       "6  The US president says protecting the Korean bo...  \n",
       "7  Donald Trump's attempt to dismiss a lawsuit al...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_text_into_new_column(df, column_name, new_column_name):\n",
    "    temp_column = []\n",
    "    for i in range(len(df)):\n",
    "        text = df[column_name].iloc[i]\n",
    "        tags = [\"assetId\", \"assetTypeCode\", \"categoryId\", \"categoryName\", \n",
    "                \"crosshead\", \"changeQueueId\", \"embed\", \"hasShortForm\", \"language\", \n",
    "                \"provider\", \"publicationStatus\", \"workerCallingCard\"]\n",
    "        for tag in tags:\n",
    "            tag_regex = '<'+ re.escape(tag) + '.*?' + re.escape(tag) + '>'\n",
    "            text = re.sub(tag_regex, '', text)\n",
    "        text = re.sub('<.*?>', '', text)\n",
    "        timestamp_regex = '\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+\\d{2}:\\d{2}'\n",
    "        text = re.sub(timestamp_regex, '', text)\n",
    "        text = re.sub('&amp;', '&', text)\n",
    "        text = re.sub('\\.\\.\\.', ' ', text)\n",
    "        whitespace_after_full_stop_regex = '(?<=[a-z])\\.(?=[A-Z])'\n",
    "        text = re.sub(whitespace_after_full_stop_regex, '. ', text)\n",
    "        whitespace_after_question_mark_regex = '(?<=[a-z])\\?(?=[A-Z])'\n",
    "        text = re.sub(whitespace_after_question_mark_regex, '. ', text)\n",
    "        temp_column.append(text)\n",
    "    df[new_column_name] = temp_column\n",
    "\n",
    "extract_text_into_new_column(cc, 'body', 'cleanBody')\n",
    "extract_text_into_new_column(trump, 'body', 'cleanBody')\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump.lastPublished = pd.to_datetime(trump.lastPublished)\n",
    "cc.lastPublished = pd.to_datetime(cc.lastPublished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all much of the data has been removed, the indexes need to be reset. I reset the indexes because, as shown in the dataframe above, the indexes of deleted rows have been removed leaving gaps in the index. The data will be easier to work with once it has been reindexed.\n",
    "\n",
    "The trump data, as pointed out earlier, appears to needs to be 'reversed' because the dates run in the opposite order to Climate Change. Both dataframes will be sorted in date order as they are reindexed because this is likely to make working with the datasets easier later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>cleanBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/news/business-38391034</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>India's double first in climate battle</td>\n",
       "      <td>2017-01-08 14:55:00</td>\n",
       "      <td>India opens two world-leading clean energy pro...</td>\n",
       "      <td>BBC News - India's double first in climate battle</td>\n",
       "      <td>Two world-leading clean energy projects have o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/news/uk-wales-38600431</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>£3m to protect Welsh and Irish coast from clim...</td>\n",
       "      <td>2017-01-13 12:48:42</td>\n",
       "      <td>Coastal tourist sites most affected by climate...</td>\n",
       "      <td>BBC News - £3m to protect Welsh and Irish coas...</td>\n",
       "      <td>Coastal tourist sites most affected by climate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/news/science-environment-38745937</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Defining a true 'pre-industrial' climate period</td>\n",
       "      <td>2017-01-25 23:11:25</td>\n",
       "      <td>Scientists suggest pushing back a baseline fro...</td>\n",
       "      <td>BBC News - Defining a true 'pre-industrial' cl...</td>\n",
       "      <td>Scientists are seeking to define a new baselin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/news/science-environment-39053678</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Most wood energy schemes are a 'disaster' for ...</td>\n",
       "      <td>2017-04-23 14:02:03</td>\n",
       "      <td>A new report says that using wood pellets to g...</td>\n",
       "      <td>BBC News - Most wood energy schemes are a 'dis...</td>\n",
       "      <td>Using wood pellets to generate low-carbon elec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/news/uk-39679584</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>March for Science: Thousands in London join gl...</td>\n",
       "      <td>2017-05-01 20:19:03</td>\n",
       "      <td>Organisers say the growth of fake news makes i...</td>\n",
       "      <td>BBC News - March for Science: Thousands in Lon...</td>\n",
       "      <td>Thousands of people have gathered in London to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             assetUri  \\\n",
       "0             /news/business-38391034   \n",
       "2             /news/uk-wales-38600431   \n",
       "3  /news/science-environment-38745937   \n",
       "4  /news/science-environment-39053678   \n",
       "8                   /news/uk-39679584   \n",
       "\n",
       "                                                body  \\\n",
       "0  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "8  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline       lastPublished  \\\n",
       "0             India's double first in climate battle 2017-01-08 14:55:00   \n",
       "2  £3m to protect Welsh and Irish coast from clim... 2017-01-13 12:48:42   \n",
       "3    Defining a true 'pre-industrial' climate period 2017-01-25 23:11:25   \n",
       "4  Most wood energy schemes are a 'disaster' for ... 2017-04-23 14:02:03   \n",
       "8  March for Science: Thousands in London join gl... 2017-05-01 20:19:03   \n",
       "\n",
       "                                             summary  \\\n",
       "0  India opens two world-leading clean energy pro...   \n",
       "2  Coastal tourist sites most affected by climate...   \n",
       "3  Scientists suggest pushing back a baseline fro...   \n",
       "4  A new report says that using wood pellets to g...   \n",
       "8  Organisers say the growth of fake news makes i...   \n",
       "\n",
       "                                               title  \\\n",
       "0  BBC News - India's double first in climate battle   \n",
       "2  BBC News - £3m to protect Welsh and Irish coas...   \n",
       "3  BBC News - Defining a true 'pre-industrial' cl...   \n",
       "4  BBC News - Most wood energy schemes are a 'dis...   \n",
       "8  BBC News - March for Science: Thousands in Lon...   \n",
       "\n",
       "                                           cleanBody  \n",
       "0  Two world-leading clean energy projects have o...  \n",
       "2  Coastal tourist sites most affected by climate...  \n",
       "3  Scientists are seeking to define a new baselin...  \n",
       "4  Using wood pellets to generate low-carbon elec...  \n",
       "8  Thousands of people have gathered in London to...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_by_date_and_reindex(df, date_column):\n",
    "    df = df.sort_values(by=[date_column])\n",
    "    df = df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sort_by_date_and_reindex(cc, 'lastPublished')\n",
    "sort_by_date_and_reindex(trump, 'lastPublished')\n",
    "cc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>cleanBody</th>\n",
       "      <th>bodyLen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/news/world-us-canada-43606088</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Donald Trump steps up attacks on Amazon</td>\n",
       "      <td>2018-03-31 21:23:32</td>\n",
       "      <td>The president accuses the online retail giant ...</td>\n",
       "      <td>BBC News - Donald Trump steps up attacks on Am...</td>\n",
       "      <td>President Donald Trump has stepped up his atta...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/news/world-us-canada-43601557</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>US may tie social media to visa applications</td>\n",
       "      <td>2018-03-31 12:55:24</td>\n",
       "      <td>A state department proposal could require visa...</td>\n",
       "      <td>BBC News - US may tie social media to visa app...</td>\n",
       "      <td>The Trump administration has said it wants to ...</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/news/world-us-canada-43578462</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Is Trump ready for talks with North Korea?</td>\n",
       "      <td>2018-03-30 21:20:28</td>\n",
       "      <td>What preparations is the White House making fo...</td>\n",
       "      <td>BBC News - Is Trump ready for talks with North...</td>\n",
       "      <td>Never before has a US president prepared for a...</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/news/world-us-canada-43591904</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump: Billions spent protecting other countri...</td>\n",
       "      <td>2018-03-29 19:31:50</td>\n",
       "      <td>Mr Trump says securing the Korean border comes...</td>\n",
       "      <td>BBC News - Trump: Billions spent protecting ot...</td>\n",
       "      <td>The US president says protecting the Korean bo...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/news/world-us-canada-43577444</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Trump loses bid to dismiss hotel lawsuit</td>\n",
       "      <td>2018-03-29 02:13:35</td>\n",
       "      <td>A judge allows a case to determine if the pres...</td>\n",
       "      <td>BBC News - Trump loses bid to dismiss hotel la...</td>\n",
       "      <td>Donald Trump's attempt to dismiss a lawsuit al...</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         assetUri  \\\n",
       "2  /news/world-us-canada-43606088   \n",
       "3  /news/world-us-canada-43601557   \n",
       "4  /news/world-us-canada-43578462   \n",
       "6  /news/world-us-canada-43591904   \n",
       "7  /news/world-us-canada-43577444   \n",
       "\n",
       "                                                body  \\\n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "6  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "7  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline       lastPublished  \\\n",
       "2            Donald Trump steps up attacks on Amazon 2018-03-31 21:23:32   \n",
       "3       US may tie social media to visa applications 2018-03-31 12:55:24   \n",
       "4         Is Trump ready for talks with North Korea? 2018-03-30 21:20:28   \n",
       "6  Trump: Billions spent protecting other countri... 2018-03-29 19:31:50   \n",
       "7           Trump loses bid to dismiss hotel lawsuit 2018-03-29 02:13:35   \n",
       "\n",
       "                                             summary  \\\n",
       "2  The president accuses the online retail giant ...   \n",
       "3  A state department proposal could require visa...   \n",
       "4  What preparations is the White House making fo...   \n",
       "6  Mr Trump says securing the Korean border comes...   \n",
       "7  A judge allows a case to determine if the pres...   \n",
       "\n",
       "                                               title  \\\n",
       "2  BBC News - Donald Trump steps up attacks on Am...   \n",
       "3  BBC News - US may tie social media to visa app...   \n",
       "4  BBC News - Is Trump ready for talks with North...   \n",
       "6  BBC News - Trump: Billions spent protecting ot...   \n",
       "7  BBC News - Trump loses bid to dismiss hotel la...   \n",
       "\n",
       "                                           cleanBody  bodyLen  \n",
       "2  President Donald Trump has stepped up his atta...      320  \n",
       "3  The Trump administration has said it wants to ...      623  \n",
       "4  Never before has a US president prepared for a...     1007  \n",
       "6  The US president says protecting the Korean bo...       20  \n",
       "7  Donald Trump's attempt to dismiss a lawsuit al...      330  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why this method?\n",
    "# though we are splitting on an empty space to get the word count. This method is likely to give a good enough value\n",
    "# because the paragraphs were marked with a <p> tag which has now been stripped out rather than a new line\n",
    "# there are more accurate ways of obtaining a word count, however this is accurate enough for \n",
    "# the initial investigation that is this coursework\n",
    "\n",
    "def create_column_containing_word_count(df, column_name, new_column_name):\n",
    "    temp_column = []\n",
    "    for i in range(len(df)):\n",
    "        current_body = len(df[column_name].iloc[i].split(' '))\n",
    "        temp_column.append(current_body)\n",
    "    df[new_column_name] = temp_column\n",
    "    \n",
    "create_column_containing_word_count(cc, 'cleanBody', 'bodyLen')\n",
    "create_column_containing_word_count(trump, 'cleanBody', 'bodyLen')\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetUri</th>\n",
       "      <th>body</th>\n",
       "      <th>headline</th>\n",
       "      <th>lastPublished</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>cleanBody</th>\n",
       "      <th>bodyLen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/news/business-38391034</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>India's double first in climate battle</td>\n",
       "      <td>2017-01-08 14:55:00</td>\n",
       "      <td>India opens two world-leading clean energy pro...</td>\n",
       "      <td>BBC News - India's double first in climate battle</td>\n",
       "      <td>Two world-leading clean energy projects have o...</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/news/uk-wales-38600431</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>£3m to protect Welsh and Irish coast from clim...</td>\n",
       "      <td>2017-01-13 12:48:42</td>\n",
       "      <td>Coastal tourist sites most affected by climate...</td>\n",
       "      <td>BBC News - £3m to protect Welsh and Irish coas...</td>\n",
       "      <td>Coastal tourist sites most affected by climate...</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/news/science-environment-38745937</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Defining a true 'pre-industrial' climate period</td>\n",
       "      <td>2017-01-25 23:11:25</td>\n",
       "      <td>Scientists suggest pushing back a baseline fro...</td>\n",
       "      <td>BBC News - Defining a true 'pre-industrial' cl...</td>\n",
       "      <td>Scientists are seeking to define a new baselin...</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/news/science-environment-39053678</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>Most wood energy schemes are a 'disaster' for ...</td>\n",
       "      <td>2017-04-23 14:02:03</td>\n",
       "      <td>A new report says that using wood pellets to g...</td>\n",
       "      <td>BBC News - Most wood energy schemes are a 'dis...</td>\n",
       "      <td>Using wood pellets to generate low-carbon elec...</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/news/uk-39679584</td>\n",
       "      <td>&lt;body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...</td>\n",
       "      <td>March for Science: Thousands in London join gl...</td>\n",
       "      <td>2017-05-01 20:19:03</td>\n",
       "      <td>Organisers say the growth of fake news makes i...</td>\n",
       "      <td>BBC News - March for Science: Thousands in Lon...</td>\n",
       "      <td>Thousands of people have gathered in London to...</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             assetUri  \\\n",
       "0             /news/business-38391034   \n",
       "2             /news/uk-wales-38600431   \n",
       "3  /news/science-environment-38745937   \n",
       "4  /news/science-environment-39053678   \n",
       "8                   /news/uk-39679584   \n",
       "\n",
       "                                                body  \\\n",
       "0  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "2  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "3  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "4  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "8  <body xmlns=\"http://www.bbc.co.uk/asset\" xml:s...   \n",
       "\n",
       "                                            headline       lastPublished  \\\n",
       "0             India's double first in climate battle 2017-01-08 14:55:00   \n",
       "2  £3m to protect Welsh and Irish coast from clim... 2017-01-13 12:48:42   \n",
       "3    Defining a true 'pre-industrial' climate period 2017-01-25 23:11:25   \n",
       "4  Most wood energy schemes are a 'disaster' for ... 2017-04-23 14:02:03   \n",
       "8  March for Science: Thousands in London join gl... 2017-05-01 20:19:03   \n",
       "\n",
       "                                             summary  \\\n",
       "0  India opens two world-leading clean energy pro...   \n",
       "2  Coastal tourist sites most affected by climate...   \n",
       "3  Scientists suggest pushing back a baseline fro...   \n",
       "4  A new report says that using wood pellets to g...   \n",
       "8  Organisers say the growth of fake news makes i...   \n",
       "\n",
       "                                               title  \\\n",
       "0  BBC News - India's double first in climate battle   \n",
       "2  BBC News - £3m to protect Welsh and Irish coas...   \n",
       "3  BBC News - Defining a true 'pre-industrial' cl...   \n",
       "4  BBC News - Most wood energy schemes are a 'dis...   \n",
       "8  BBC News - March for Science: Thousands in Lon...   \n",
       "\n",
       "                                           cleanBody  bodyLen  \n",
       "0  Two world-leading clean energy projects have o...      764  \n",
       "2  Coastal tourist sites most affected by climate...      242  \n",
       "3  Scientists are seeking to define a new baselin...      822  \n",
       "4  Using wood pellets to generate low-carbon elec...      991  \n",
       "8  Thousands of people have gathered in London to...      451  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(cc.lastPublished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(trump.lastPublished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining\n",
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/leives01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//TODO\n",
    "* Tokenize each article into sentences. Each sentence is called a token. \n",
    "* Remove stop words -- stop words are very common words that don't really give any extra information regarding the meaning of the text ('a','as','and' etc.).\n",
    "* Identify n-grams (bigrams)\n",
    "* stemming similar words e.g. close, closing and closed \n",
    "* Part of Speech (POS) tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text\n",
    "I start by breaking the `cleanBody` into sentences and tokenizing them which means removing common stop words such as 'the', 'and', 'a' and 'that'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# sent = sent_tokenize(cc['cleanBody'].iloc[0])\n",
    "\n",
    "# sent = \"the cat sat on the mat. The cat did this. The cat did that.\"\n",
    "# print(sent_tokenize(sent))\n",
    "\n",
    "# matrix = []\n",
    "# for i in range(len(cc)):\n",
    "#     current = cc['cleanBody'].iloc[0]\n",
    "#     sent = sent_tokenize(current)\n",
    "#     matrix.append(sent)\n",
    "    \n",
    "# print(matrix)\n",
    "\n",
    "def make_matrix_of_sentences(df, column):\n",
    "    matrix = []\n",
    "    for i in range(len(df)):\n",
    "        current = df[column].iloc[i]\n",
    "        sent = sent_tokenize(current)\n",
    "        matrix.append(sent)\n",
    "    return matrix\n",
    "\n",
    "ccMatrix = make_matrix_of_sentences(cc, 'cleanBody')\n",
    "trumpMatrix = make_matrix_of_sentences(trump, 'cleanBody')\n",
    "# ccMatrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Australia's Great Barrier Reef can be saved only if urgent steps are taken to reduce global warming, new research has warned.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccMatrix[100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Australia\\'s Great Barrier Reef can be saved only if urgent steps are taken to reduce global warming, new research has warned. Attempting to stop coral bleaching through any other method will not be sufficient, according to scientists. The research, published in the journal Nature, said bleaching events should no longer be studied individually, but as threats to the reef\\'s survival. The bleaching - or loss of algae - in 2016 was the worst on record.\"Climate change is the single greatest threat to the Great Barrier Reef,\" said co-author Prof Morgan Pratchett, from Queensland\\'s James Cook University.\"It all comes down to what the governments in Australia and around the world do in terms of mitigating further rises in temperatures.\"     What causes coral bleaching?        Coral bleaching is caused by rising water temperatures resulting from two natural warm currents.    It is exacerbated by man-made climate change, as the oceans are absorbing about 93% of the increase in the Earth\\'s heat.    Bleaching happens when corals under stress drive out the algae known as zooxanthellae that give them colour.    If normal conditions return, the corals can recover, but it can take decades, and if the stress continues the corals can die.  Lead author Prof Terry Hughes warned bleaching events had become \"the new normal\".Last week, he said an aerial survey had shown evidence of mass bleaching in consecutive summers for the first time. The scale of the damage will be examined in the next three weeks by the National Coral Bleaching Taskforce, a collaboration of scientists and reef managers.      Australian summer broke 205 records        Can super coral save the Great Barrier Reef?    Is the Great Barrier Reef coming to the end of its life?  Prof Pratchett said he remained optimistic the reef could recover, but the \"window of opportunity\" to curb emissions was closing. \"It\\'s the number one thing we need to think about now to save the reef,\" he told the BBC.Improving fishing practices or water quality would not be enough, he said.    The second bleaching is causing concerns over the reef\\'s long term health.  The reef - a vast collection of thousands of smaller coral reefs stretching from the northern tip of Queensland to the state\\'s southern city of Bundaberg - was given World Heritage status in 1981.The UN says it is the \"most biodiverse\" of all the World Heritage sites, and of \"enormous scientific and intrinsic importance\".'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc['cleanBody'].iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punkt includes stopwords in different languages\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "custom_stop_words = [\"'s\", \"``\", \"''\", \"\\\"\"]\n",
    "stop_words = set(stopwords.words('english') + list(punctuation) + list(custom_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(df, column):\n",
    "    words_matrix = []\n",
    "    for i in range (len(df)):\n",
    "        current = df[column].iloc[i]\n",
    "        these_words = [word for word in word_tokenize(current) if word not in stop_words]\n",
    "        these_words\n",
    "        words_matrix.append(these_words)\n",
    "    return words_matrix\n",
    "\n",
    "ccTokens = tokenize_words(cc, 'cleanBody')\n",
    "trumpTokens = tokenize_words(trump, 'cleanBody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['President', 'Donald', 'Trump', \"'Nambia\", 'Africa', \"'business\", 'potential', 'Nambia', 'health', 'system', 'increasingly', 'self-sufficient', 'said', 'US', 'President', 'Donald', 'Trump', 'lunch', 'African', 'leaders', 'New', 'York', 'Wednesday', 'reeling', 'list', 'nations', 'achievements', 'But', 'country', 'exists', 'Could', 'US', 'leader', 'referring', 'Namibia', 'Zambia', 'Or', 'perhaps', 'The', 'Gambia', 'Social', 'media', 'users', 'wasted', 'time', 'offering', 'explanations', 'One', 'person', 'shared', 'image', 'US', 'race', 'activist', 'Rachel', 'Dolezal', 'identifies', 'black', 'despite', 'born', 'white', 'recently', 'visited', 'South', 'Africa', 'Find', 'Namibia', 'Zambia', 'The', 'Gambia', 'Namibia', 'country', 'profileNamibia', 'country', 'profileProvides', 'overview', 'Namibia', 'including', 'key', 'events', 'facts', 'sparsely', 'populated', 'stable', 'country', 'Africa', 'south-west', 'coast', 'BBC', 'News', 'Namibia', 'country', 'profileZambia', 'country', 'profileZambia', 'profileProvides', 'overview', 'key', 'facts', 'events', 'timelines', 'leader', 'profiles', 'along', 'current', 'news', 'Zambia', 'BBC', 'News', 'Zambia', 'country', 'profileThe', 'Gambia', 'country', 'profileThe', 'Gambia', 'profileProvides', 'overview', 'key', 'facts', 'events', 'timelines', 'leader', 'profiles', 'along', 'current', 'news', 'The', 'Gambia', 'BBC', 'News', 'The', 'Gambia', 'country', 'profileAfrica', 'Live', 'More', 'storiesBBC', 'Africa', 'Live', 'pageNamibia', 'President', 'Hage', 'Geingob', 'present', 'Mr', 'Trump', 'made', 'gaffe', 'He', 'yet', 'commented', 'Some', 'Twitter', 'users', 'also', 'bristled', 'President', 'Trump', 'comments', 'Africa', 'tremendous', 'business', 'potential', 'I', 'many', 'friends', 'going', 'countries', 'trying', 'get', 'rich', 'I', 'congratulate', \"'re\", 'spending', 'lot', 'money', 'Mr', 'Trump', 'said', 'It', 'represents', 'huge', 'amounts', 'different', 'markets', 'It', 'really', 'become', 'place', 'go', 'want', 'go', 'Others', 'defended', 'Mr', 'Trump', 'saying', 'aside', 'Nambia', 'blunder', 'made', 'valid', 'points', 'Iranian', 'interpreter', 'defends', 'Trump', 'speech', 'omissionsInterpreter', 'defends', 'Trump', 'speech', 'omissionsNima', 'Chitsaz', 'misinterpreted', 'parts', 'Trump', 'UN', 'speech', 'said', 'untrue', 'Iran', 'BBC', 'News', 'Iranian', 'interpreter', 'defends', 'Trump', 'speech', 'omissions']\n"
     ]
    }
   ],
   "source": [
    "print(trumpTokens[1010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "def get_bigrams(tokens_list):\n",
    "    values = []\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    for i in range(len(tokens_list)):\n",
    "        finder = BigramCollocationFinder.from_words(tokens_list[i])\n",
    "        bigrams = list(finder.ngram_fd.items())\n",
    "        bigrams.sort(key=lambda item: item[-1], reverse=True)\n",
    "        values.append(bigrams)\n",
    "    return values\n",
    "\n",
    "trump_bigrams = get_bigrams(trumpTokens)\n",
    "cc_bigrams = get_bigrams(ccTokens)\n",
    "# trigram_finder = TrigramCollocationFinder.from_words(trumpTokens[1010])\n",
    "# trump_trigrams = list(trigram_finder.ngram_fd.items())\n",
    "# trump_trigrams.sort(key=lambda item: item[-1], reverse=True)\n",
    "# trump_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(tokens_list):\n",
    "    values = []\n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "    for i in range(len(tokens_list)):\n",
    "        finder = TrigramCollocationFinder.from_words(tokens_list[i])\n",
    "        trigrams = list(finder.ngram_fd.items())\n",
    "        trigrams.sort(key=lambda item: item[-1], reverse=True)\n",
    "        values.append(trigrams)\n",
    "    return values\n",
    "\n",
    "trump_trigrams = get_trigrams(trumpTokens)\n",
    "cc_trigrams = get_trigrams(ccTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def get_pos_tags(tokens_list):\n",
    "    values = []\n",
    "    for i in range(len(tokens_list)):\n",
    "        current_list = tokens_list[i]\n",
    "        values.append(nltk.pos_tag(current_list))\n",
    "    return values\n",
    "\n",
    "cc_pos_tags = get_pos_tags(ccTokens)\n",
    "trump_pos_tags = get_pos_tags(trumpTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_processed_df = pd.DataFrame({\n",
    "    'sentences': trumpMatrix, \n",
    "    'tokenized_words': trumpTokens,\n",
    "    'bigrams': trump_bigrams,\n",
    "    'trigrams': trump_trigrams,\n",
    "    'pos_tags': trump_pos_tags\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[((Post, Office), 3), ((Mr, Trump), 3), ((atta...</td>\n",
       "      <td>[(President, NNP), (Donald, NNP), (Trump, NNP)...</td>\n",
       "      <td>[President Donald Trump has stepped up his att...</td>\n",
       "      <td>[President, Donald, Trump, stepped, attacks, A...</td>\n",
       "      <td>[((US, Postal, Service), 2), ((US, Post, Offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[((social, media), 4), ((visa, applicants), 4)...</td>\n",
       "      <td>[(The, DT), (Trump, NNP), (administration, NN)...</td>\n",
       "      <td>[The Trump administration has said it wants to...</td>\n",
       "      <td>[The, Trump, administration, said, wants, star...</td>\n",
       "      <td>[((New, York, Times), 2), ((Trump, 'in, crude)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[((North, Korea), 12), ((Korea, talks), 5), ((...</td>\n",
       "      <td>[(Never, RB), (US, NNP), (president, NN), (pre...</td>\n",
       "      <td>[Never before has a US president prepared for ...</td>\n",
       "      <td>[Never, US, president, prepared, summit, impor...</td>\n",
       "      <td>[((Who, going, represent), 3), ((going, repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[((The, US), 1), ((US, president), 1), ((presi...</td>\n",
       "      <td>[(The, DT), (US, NNP), (president, NN), (says,...</td>\n",
       "      <td>[The US president says protecting the Korean b...</td>\n",
       "      <td>[The, US, president, says, protecting, Korean,...</td>\n",
       "      <td>[((The, US, president), 1), ((US, president, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[((Mr, Trump), 4), ((Trump, International), 3)...</td>\n",
       "      <td>[(Donald, NNP), (Trump, NNP), (attempt, NN), (...</td>\n",
       "      <td>[Donald Trump's attempt to dismiss a lawsuit a...</td>\n",
       "      <td>[Donald, Trump, attempt, dismiss, lawsuit, all...</td>\n",
       "      <td>[((Trump, International, Hotel), 3), ((Lawyers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             bigrams  \\\n",
       "0  [((Post, Office), 3), ((Mr, Trump), 3), ((atta...   \n",
       "1  [((social, media), 4), ((visa, applicants), 4)...   \n",
       "2  [((North, Korea), 12), ((Korea, talks), 5), ((...   \n",
       "3  [((The, US), 1), ((US, president), 1), ((presi...   \n",
       "4  [((Mr, Trump), 4), ((Trump, International), 3)...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(President, NNP), (Donald, NNP), (Trump, NNP)...   \n",
       "1  [(The, DT), (Trump, NNP), (administration, NN)...   \n",
       "2  [(Never, RB), (US, NNP), (president, NN), (pre...   \n",
       "3  [(The, DT), (US, NNP), (president, NN), (says,...   \n",
       "4  [(Donald, NNP), (Trump, NNP), (attempt, NN), (...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [President Donald Trump has stepped up his att...   \n",
       "1  [The Trump administration has said it wants to...   \n",
       "2  [Never before has a US president prepared for ...   \n",
       "3  [The US president says protecting the Korean b...   \n",
       "4  [Donald Trump's attempt to dismiss a lawsuit a...   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [President, Donald, Trump, stepped, attacks, A...   \n",
       "1  [The, Trump, administration, said, wants, star...   \n",
       "2  [Never, US, president, prepared, summit, impor...   \n",
       "3  [The, US, president, says, protecting, Korean,...   \n",
       "4  [Donald, Trump, attempt, dismiss, lawsuit, all...   \n",
       "\n",
       "                                            trigrams  \n",
       "0  [((US, Postal, Service), 2), ((US, Post, Offic...  \n",
       "1  [((New, York, Times), 2), ((Trump, 'in, crude)...  \n",
       "2  [((Who, going, represent), 3), ((going, repres...  \n",
       "3  [((The, US, president), 1), ((US, president, s...  \n",
       "4  [((Trump, International, Hotel), 3), ((Lawyers...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_processed_df = pd.DataFrame({\n",
    "    'sentences': ccMatrix, \n",
    "    'tokenized_words': ccTokens,\n",
    "    'bigrams': cc_bigrams,\n",
    "    'trigrams': cc_trigrams,\n",
    "    'pos_tags': cc_pos_tags\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>[((away, coal), 4), ((climate, change), 4), ((...</td>\n",
       "      <td>[(The, DT), (UK, NNP), (Canada, NNP), (launche...</td>\n",
       "      <td>[The UK and Canada have launched a global alli...</td>\n",
       "      <td>[The, UK, Canada, launched, global, alliance, ...</td>\n",
       "      <td>[((time, next, major), 2), ((next, major, UN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>[((food, waste), 9), ((surplus, food), 5), ((B...</td>\n",
       "      <td>[(A, DT), (rising, VBG), (number, NN), (firms,...</td>\n",
       "      <td>[A rising number of firms are finding creative...</td>\n",
       "      <td>[A, rising, number, firms, finding, creative, ...</td>\n",
       "      <td>[((issue, food, waste), 2), ((A, rising, numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>[((Meeting, carbon-reduction), 1), ((carbon-re...</td>\n",
       "      <td>[(Meeting, VBG), (carbon-reduction, JJ), (targ...</td>\n",
       "      <td>[Meeting carbon-reduction targets will be more...</td>\n",
       "      <td>[Meeting, carbon-reduction, targets, challengi...</td>\n",
       "      <td>[((Meeting, carbon-reduction, targets), 1), ((...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>[((More, hurricanes), 1), ((hurricanes, Rising...</td>\n",
       "      <td>[(More, JJR), (hurricanes, NNS), (Rising, VBG)...</td>\n",
       "      <td>[More hurricanes?, Rising temperatures?, BBC m...</td>\n",
       "      <td>[More, hurricanes, Rising, temperatures, BBC, ...</td>\n",
       "      <td>[((More, hurricanes, Rising), 1), ((hurricanes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>[((UK, US), 1), ((US, scientists), 1), ((scien...</td>\n",
       "      <td>[(UK, NNP), (US, NNP), (scientists, NNS), (lea...</td>\n",
       "      <td>[UK and US scientists will lead a five-year ef...</td>\n",
       "      <td>[UK, US, scientists, lead, five-year, effort, ...</td>\n",
       "      <td>[((UK, US, scientists), 1), ((US, scientists, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               bigrams  \\\n",
       "657  [((away, coal), 4), ((climate, change), 4), ((...   \n",
       "658  [((food, waste), 9), ((surplus, food), 5), ((B...   \n",
       "659  [((Meeting, carbon-reduction), 1), ((carbon-re...   \n",
       "660  [((More, hurricanes), 1), ((hurricanes, Rising...   \n",
       "661  [((UK, US), 1), ((US, scientists), 1), ((scien...   \n",
       "\n",
       "                                              pos_tags  \\\n",
       "657  [(The, DT), (UK, NNP), (Canada, NNP), (launche...   \n",
       "658  [(A, DT), (rising, VBG), (number, NN), (firms,...   \n",
       "659  [(Meeting, VBG), (carbon-reduction, JJ), (targ...   \n",
       "660  [(More, JJR), (hurricanes, NNS), (Rising, VBG)...   \n",
       "661  [(UK, NNP), (US, NNP), (scientists, NNS), (lea...   \n",
       "\n",
       "                                             sentences  \\\n",
       "657  [The UK and Canada have launched a global alli...   \n",
       "658  [A rising number of firms are finding creative...   \n",
       "659  [Meeting carbon-reduction targets will be more...   \n",
       "660  [More hurricanes?, Rising temperatures?, BBC m...   \n",
       "661  [UK and US scientists will lead a five-year ef...   \n",
       "\n",
       "                                       tokenized_words  \\\n",
       "657  [The, UK, Canada, launched, global, alliance, ...   \n",
       "658  [A, rising, number, firms, finding, creative, ...   \n",
       "659  [Meeting, carbon-reduction, targets, challengi...   \n",
       "660  [More, hurricanes, Rising, temperatures, BBC, ...   \n",
       "661  [UK, US, scientists, lead, five-year, effort, ...   \n",
       "\n",
       "                                              trigrams  \n",
       "657  [((time, next, major), 2), ((next, major, UN),...  \n",
       "658  [((issue, food, waste), 2), ((A, rising, numbe...  \n",
       "659  [((Meeting, carbon-reduction, targets), 1), ((...  \n",
       "660  [((More, hurricanes, Rising), 1), ((hurricanes...  \n",
       "661  [((UK, US, scientists), 1), ((US, scientists, ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_processed_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump mean: 556.5772307692308 \n",
      " cc mean: 550.9365558912386\n",
      "trump median: 508.0 \n",
      " cc median: 553.0\n",
      "trump mode: ModeResult(mode=array([25]), count=array([27])) \n",
      " cc mode: ModeResult(mode=array([44]), count=array([10]))\n",
      "55.88307692307692\n",
      "58.84592145015106\n"
     ]
    }
   ],
   "source": [
    "def column_of_adjectives_and_adverbs(df, column, new_column_name):\n",
    "    tags_of_interest = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "    temp_column = []\n",
    "    for d in range(len(df[column])):\n",
    "        pos_tags = df[column].iloc[d]\n",
    "        adj_and_advs = []\n",
    "        for i in range(len(pos_tags)):\n",
    "            word = pos_tags[i][0]\n",
    "            tag = pos_tags[i][1] \n",
    "            if tag in tags_of_interest:\n",
    "                adj_and_advs.append(word)\n",
    "        temp_column.append(adj_and_advs)\n",
    "    df[new_column_name] = temp_column\n",
    "\n",
    "column_of_adjectives_and_adverbs(cc_processed_df, 'pos_tags', 'adj_and_advs')\n",
    "column_of_adjectives_and_adverbs(trump_processed_df, 'pos_tags', 'adj_and_advs')\n",
    "\n",
    "\n",
    "def get_list_size(l):\n",
    "    temp = []\n",
    "    for i in range(len(l)):\n",
    "        temp.append(len(l[i]))\n",
    "    return temp\n",
    "\n",
    "def get_percent(num, out_of):\n",
    "    return (num / out_of) * 100\n",
    "\n",
    "cc_adj_adv_df = pd.DataFrame({\n",
    "    'total_words': cc['bodyLen'],\n",
    "    'adj_and_advs': get_list_size(cc_processed_df['adj_and_advs']),\n",
    "    'percentage': get_percent(cc_adj_adv_df['adj_and_advs'], cc_adj_adv_df['total_words'])\n",
    "})\n",
    "\n",
    "trump_adj_adv_df = pd.DataFrame({\n",
    "    'total_words': trump['bodyLen'],\n",
    "    'adj_and_advs': get_list_size(trump_processed_df['adj_and_advs']),\n",
    "    'percentage': get_percent(trump_adj_adv_df['adj_and_advs'], trump_adj_adv_df['total_words'])\n",
    "})   \n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#mean value\n",
    "trump_mean_words= np.mean(trump_adj_adv_df['total_words'])\n",
    "cc_mean_words= np.mean(cc_adj_adv_df['total_words'])\n",
    "\n",
    "trump_mean_adj = np.mean(trump_adj_adv_df['adj_and_advs'])\n",
    "cc_mean_adj = np.mean(cc_adj_adv_df['adj_and_advs'])\n",
    "\n",
    "#median value\n",
    "trump_median_words = np.median(trump_adj_adv_df['total_words'])\n",
    "cc_median_words = np.median(cc_adj_adv_df['total_words'])\n",
    "\n",
    "#mode value\n",
    "# With scipy, an array, ModeResult, is returned that has 2 attributes. \n",
    "# The first attribute, mode, is the number that is the mode of the data set. \n",
    "# The second attribute, count, is the number of times it occurs in the data set.\n",
    "trump_mode_words = stats.mode(trump_adj_adv_df['total_words'])\n",
    "cc_mode_words = stats.mode(cc_adj_adv_df['total_words'])\n",
    "\n",
    "print('trump mean: {0} \\n cc mean: {1}'.format(trump_mean_words, cc_mean_words))\n",
    "print('trump median: {0} \\n cc median: {1}'.format(trump_median_words, cc_median_words))\n",
    "print('trump mode: {0} \\n cc mode: {1}'.format(trump_mode_words, cc_mode_words))\n",
    "print(trump_mean_adj)\n",
    "print(cc_mean_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj_and_advs</th>\n",
       "      <th>percentage</th>\n",
       "      <th>total_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>11.649215</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>6.198347</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124</td>\n",
       "      <td>15.085158</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106</td>\n",
       "      <td>10.696266</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38</td>\n",
       "      <td>8.425721</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adj_and_advs  percentage  total_words\n",
       "0            89   11.649215          764\n",
       "2            15    6.198347          242\n",
       "3           124   15.085158          822\n",
       "4           106   10.696266          991\n",
       "8            38    8.425721          451"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_adj_adv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj_and_advs</th>\n",
       "      <th>percentage</th>\n",
       "      <th>total_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>7.187500</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>9.630819</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112</td>\n",
       "      <td>11.122145</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>5.151515</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adj_and_advs  percentage  total_words\n",
       "2            23    7.187500          320\n",
       "3            60    9.630819          623\n",
       "4           112   11.122145         1007\n",
       "6             3   15.000000           20\n",
       "7            17    5.151515          330"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_adj_adv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "# JJ, JJR, JJS, RB, RBR, RBS\n",
    "\n",
    "# I am not interested in wh-adverbs https://www.ucl.ac.uk/internet-grammar/adverbs/wh.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3245    [US President-elect Donald Trump says there mu...\n",
       "3246    [Ford's decision to cancel a $1.6bn investment...\n",
       "3247    [A leading US Republican says she fears for th...\n",
       "3248    [2016 has been one dubbed for the history book...\n",
       "3249    [Thirty-five Russian diplomats expelled from t...\n",
       "Name: sentences, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_processed_df['sentences'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-Based Autosummarizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import FreqDist\n",
    "\n",
    "# freq = FreqDist(trump_processd_df.tokenized_words.iloc[0])\n",
    "# freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Currently, we still have two large datasets of BBC News articles; one tagged with the topic 'Donald Trump' and the other with 'Climate Change'. All articles tagged with these topics are mostly about Donald Trump or Climage Change, they will also touch on different areas also, and have different newsworthy angles and focuses. \n",
    "\n",
    "I decided that it would be good to investigate where there are any specific angles that are covered regularly. I will refer to these as 'themes' hereafter. It would interesting to see if there are any recurring themes in articles about these topics. I am asking: is something very specific written about very often under these given topics? If so, this _might_ suggest bias. \n",
    "\n",
    "Each dataset could be divided into groups, or themes, based on some measure of similarity that might be present. There is likely to be a difference in the common attributes of the Donald Trump articles compared with the Climate Change articles. All articles that represent a particular 'theme' within each dataset should be clustered into one group so they might then be compared and analysed for evidence impartiality. \n",
    "\n",
    "This is a clustering problem. Clustering is defined as grouping objects which are similar to each other and dissimilar to the objects in the other groupings (Bramer 2007). The objective in clusering is to maximise intracluster similarity and minimize intercluster similarity (Kolalapudi 2016).\n",
    "\n",
    "## Feature Extraction\n",
    "\n",
    "The first step is representing text data using numeric attributes called features and the process of extracting them from the text is called feature extraction (Kolalapudi 2016). There are various methods by which feature extraction can be performed, but the methods I shall be concerned with here are called 'Term Frequency' and 'TF-IDF'. \n",
    "\n",
    "### Term Frequency Representation using the 'Bag of Words' model.\n",
    "Steps to performing Term Frequency\n",
    "1. Create a list of all possible words that could appear any article. The list is of length `n`, where `n` is the number of words in the text. \n",
    "2. A tuple is created for each article where each word in the list of all words is represented by the number of times it occurred in the text. \n",
    "For example,\n",
    "if all the possible words are `(these, are, all, the, possible, words, that, could, appear)` then the sentence `all these words are possible` would produce the tuple `(1, 1, 1, 0, 1, 1, 0, 0, 0)` which is called the Term Frequency Representation.\n",
    "\n",
    "It is important to note all information regarding the order of the words in the text is lost, this is why this model is called 'Bag of Words' because it is likened to putting all the words in a bag (Kolalapudi 2016). This is suitable for our initial experiment in discovering themes because we will be clustering based on the words themselves not the order they appeared in. \n",
    "\n",
    "### Term Freqency - Inverse Document Frequency\n",
    "Term Freqency - Inverse Document Frequency (TF-IDF) enhances the Term Frequency Representation by recognising that some words are more important than others (Kolalapudi 2016). TF-IDF works on the widely held supposition that a seldomly used word found in a sentence generally has more of an impact on the meaning of that sentence than other words (Kolalapudi 2016). By taking this into account, I would like to a way of 'weighting' each word in my features to take into account its rarity. TF-IDF is a commonly used way of doing this. \n",
    "\n",
    "The method of finding the TF-IDF is to take the corpus, which is the entire dataset of articles, and the tuples of word frequencies, then weight each word's frequency by the inverse of the number of articles the word is present in (Kolalapudi 2016). The term frequency is multiplied by the inverse of the document frequency which is why this method is called 'Term Freqency - Inverse Document Frequency' (Kolalapudi 2016). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Corpus\n",
    "\n",
    "I start the process of feature extraction by getting the corpuses of each dataset. I am using the `tokenized_words` to do this because it is my cleanest respresentation of the data so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df, column):\n",
    "    corpus = []\n",
    "    for i in range (len(df)):\n",
    "        text = df[column].iloc[i]\n",
    "        corpus.append(' '.join(text))\n",
    "    return corpus\n",
    "\n",
    "cc_corpus = get_corpus(cc_processed_df, 'tokenized_words')\n",
    "trump_corpus = get_corpus(trump_processed_df, 'tokenized_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the TF-IDF\n",
    "\n",
    "I am using a popular Data Science library, specifically for machine learning problems called SciKit Learn. I am using  built-in methods for creating the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3250x36560 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 697087 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tfid = TfidfVectorizer().fit_transform(trump_corpus)\n",
    "trump_tfid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<662x12800 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 146686 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_tfid = TfidfVectorizer().fit_transform(cc_corpus)\n",
    "cc_tfid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "K-means clustering is described by Bramner (2007) as an 'exclusive clustering algorithm' because it assigns each object to only one of a set of clusters. This method was chosen because I am looking for distinct overarching themes that may be present within the news articles, I am not looking to put them into a hierarchy for example.\n",
    "\n",
    "Steps to performing k-means clustering (Bramner 2007).\n",
    "1. Decide on a value of `k` which is the number of clusters that should be found. \n",
    "2. Select the `k` objects in an arbitary fashion, these should be the inital set of `k` centroids. \n",
    "3. Each object is assigned one by one to the cluster which has the closet centroid. \n",
    "4. Now, it is very likely that the original centroids will no longer be the true centroids of each clusters, therefore a recalculation must be made and the process is repeated. \n",
    "5. Repeat steps 3. and 4. until the centroids do not change and convergence has been reached. \n",
    "\n",
    "What SciKit Learn referrs to as the `inertia` is what is usually referred to as the 'objective function': the sum of the squared distances of objects to their closest cluster centre (SciKit Learn 2017; Bramner 2007). The objective function should decrease with each iteration until the best clustering is found. \n",
    "\n",
    "Below the number of clusters is set to 3 and the maximum number of iterations is set to 99. I have chosen 3 from running this experiment several times and I find it to be a good number. Thankfully, the number of iterations before convergence has never gotten close to 99 in all the times I have run this experiment, but it is important to set the maximum number of iterations anyway (Bramner 2007). This is because although it can be proved that k-means clustering will always terminate, it does not always find the best set of clusters, corresponding the minimising the value of the objective function (Bramner 2007). \n",
    "\n",
    "The initial selection of centroids can too significantly affect the result (Bramner 2007). To overcome this, the algorithm can be run several times for a given value of `k`, each time with a different choice of inital `k` centroids (Bramner 2007). The set of clusters with the smallest value of the objective function is then chosen (Bramner 2007). The `n_init` value set is the 'number of time the k-means algorithm will be run with different centroid seeds' (SciKit Learn 2017). The final results from SciKit Learn are the best output of the `n_init` consecutive runs in terms of intertia (SciKit Learn 2017). From running this experiment several times and tweaking the numbers I have found that 4 is a good number of iterations to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=99,\n",
       "    n_clusters=3, n_init=4, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cc_km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 99, n_init = 4, verbose = False )\n",
    "\n",
    "cc_km.fit(cc_tfid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a simple method to print some of the findings because the method's output, when set to verbose, is too verbose.\n",
    "\n",
    "The labels show that there are 3 clusters (but we knew that because we set it) and it also shows the number of articles which have been assigned to each cluster in the second array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective funtion: 599.3456414579845\n",
      " coordinates of cluster centres:\n",
      " [[0.00361623 0.0065828  0.00339312 ... 0.         0.00041688 0.00042302]\n",
      " [0.0011834  0.00215879 0.         ... 0.         0.         0.        ]\n",
      " [0.0002421  0.00970895 0.         ... 0.00023215 0.0002061  0.        ]]\n",
      "labels: \n",
      "(array([0, 1, 2], dtype=int32), array([192, 138, 332]))\n"
     ]
    }
   ],
   "source": [
    "def print_output(km): \n",
    "    string = 'objective funtion: {0}\\n coordinates of cluster centres:\\n {1}'.format(km.inertia_, km.cluster_centers_)\n",
    "    labels = 'labels: \\n{}'.format(np.unique(km.labels_ , return_counts=True))\n",
    "    print(string)\n",
    "    print(labels)\n",
    "    \n",
    "print_output(cc_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=99,\n",
       "    n_clusters=3, n_init=4, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 99, n_init = 4, verbose = False )\n",
    "trump_km.fit(trump_tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective funtion: 3016.1153431619823\n",
      " coordinates of cluster centres:\n",
      " [[1.67059886e-03 7.67825063e-03 1.38461465e-05 ... 9.28794456e-05\n",
      "  2.56560130e-05 2.56560130e-05]\n",
      " [8.15024811e-04 2.14029934e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.95050159e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "labels: \n",
      "(array([0, 1, 2], dtype=int32), array([2686,  399,  165]))\n"
     ]
    }
   ],
   "source": [
    "print_output(trump_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the Themes\n",
    "\n",
    "The method below creates a dictionary for each corpus whereby the keys are the cluster numbers and the values are the aggregated text across all the articles that are within that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_from_corpus(km, corpus):\n",
    "    text = {}\n",
    "    for i, cluster in enumerate(km.labels_):\n",
    "        current = corpus[i]\n",
    "        if cluster not in text.keys():\n",
    "            text[cluster] = current\n",
    "        else:\n",
    "            text[cluster] += current\n",
    "    return text\n",
    "\n",
    "cc_cluster_dict = dictionary_from_corpus(cc_km, cc_corpus)\n",
    "trump_cluster_dict = dictionary_from_corpus(trump_km, trump_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to find the most frequent words within that cluster. \n",
    "\n",
    "Through running and tweaking this experiment, I found it necessary add 'bbc' to the set of `stop_words` because it was appearing too often and is not helpful in identifying themes in BBC News articles. By the end of this method, the words have been tokenized, re-joined to find the TF-IDF and then tokenized again within this method. It is likely because of this that something odd was happening: 'n't' got treated as a word and was appearing often; therefore I also added this to the `stop_words`. \n",
    "\n",
    "`freq = FreqDist(word_sent)` calculates the frequency distribution of the words and `keywords[cluster] = nlargest(99, freq, key=freq.get)` finds the most important 100 words in that cluster (set to 99 because Python is zero-based). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from heapq import nlargest\n",
    "\n",
    "stop_words.add('bbc')\n",
    "stop_words.add(\"n't\")\n",
    "\n",
    "def get_freq_dist(dict):\n",
    "    keywords = {}\n",
    "    counts = {}\n",
    "    for cluster in range(len(dict)):\n",
    "        word_sent = word_tokenize(dict[cluster].lower())\n",
    "        word_sent = [word for word in word_sent if word not in stop_words]\n",
    "        freq = FreqDist(word_sent)\n",
    "        keywords[cluster] = nlargest(99, freq, key=freq.get)\n",
    "        counts[cluster] = freq\n",
    "    return (keywords, counts)\n",
    "\n",
    "cc_word_counts = get_freq_dist(cc_cluster_dict)\n",
    "trump_word_counts = get_freq_dist(trump_cluster_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keywords that are unique to each cluster are found, and the top 12 for each corpus are shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_keys(data, num_of_unique_keys=10):\n",
    "    keywords = data[0]\n",
    "    counts = data[1]\n",
    "    unique_keys = {}\n",
    "    length = len(counts)\n",
    "    for cluster in range(length):\n",
    "        other_clusters = list(set(range(length))-set([cluster]))\n",
    "        keys_other_clusters = set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "        unique=set(keywords[cluster])-keys_other_clusters\n",
    "        unique_keys[cluster]=nlargest(num_of_unique_keys,unique,key=counts[cluster].get)\n",
    "    return unique_keys\n",
    "\n",
    "cc_12_unique_keys = get_unique_keys(cc_word_counts, 12)\n",
    "trump_12_unique_keys = get_unique_keys(trump_word_counts, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['america',\n",
       "  'order',\n",
       "  'ban',\n",
       "  'american',\n",
       "  'like',\n",
       "  'countries',\n",
       "  'back',\n",
       "  'climate',\n",
       "  'since',\n",
       "  'office',\n",
       "  'immigration',\n",
       "  'tax'],\n",
       " 1: ['russian',\n",
       "  'comey',\n",
       "  'fbi',\n",
       "  'investigation',\n",
       "  'intelligence',\n",
       "  'flynn',\n",
       "  'director',\n",
       "  'clinton',\n",
       "  'mueller',\n",
       "  'putin',\n",
       "  'team',\n",
       "  'committee'],\n",
       " 2: ['north',\n",
       "  'korea',\n",
       "  'nuclear',\n",
       "  'iran',\n",
       "  'korean',\n",
       "  'kim',\n",
       "  'missile',\n",
       "  'south',\n",
       "  'pyongyang',\n",
       "  'un',\n",
       "  'weapons',\n",
       "  'jong-un']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_12_unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['ice',\n",
       "  'water',\n",
       "  'sea',\n",
       "  'dr',\n",
       "  'ocean',\n",
       "  'project',\n",
       "  'antarctic',\n",
       "  'prof',\n",
       "  'data',\n",
       "  'glacier',\n",
       "  'researchers',\n",
       "  'university'],\n",
       " 1: ['deal',\n",
       "  'accord',\n",
       "  'china',\n",
       "  'donald',\n",
       "  'decision',\n",
       "  'house',\n",
       "  'white',\n",
       "  'pullout',\n",
       "  'administration',\n",
       "  'un',\n",
       "  'meeting',\n",
       "  'effects'],\n",
       " 2: ['scotland',\n",
       "  'co2',\n",
       "  'report',\n",
       "  'scottish',\n",
       "  'targets',\n",
       "  'plan',\n",
       "  'heat',\n",
       "  'power',\n",
       "  'action',\n",
       "  'since',\n",
       "  'need',\n",
       "  '2016']}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_12_unique_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the overarching themes in the Climate Change corpus is very interesting to us since it seems to be about Donald Trump! This might suggest that in further investigation regarding the impartiality of a topic, articles broader than those tagged with that topic under investigation should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Legal and General Investment Management (LGIM) has said it will take action against companies that are not addressing the risks of climate change.LGIM, one of the biggest investment funds in Europe, said it would exclude offending firms from its Future World index fund.Where those firms featured in its other equity funds, it would vote against re-electing the chairs of their boards.China Construction Bank and Russia\\'s Rosneft were among the worst, it said.\"China Construction Bank remains the world\\'s largest funder of coal mining and plants,\" LGIM said.\"While the company has increased its lending to green projects, it does not disclose the total [greenhouse gas] emissions associated with its business.\"Rosneft, the Russian oil giant, was branded a laggard for producing a 144-page sustainability report that did not mention \"climate change\" once.\"This provides little reassurance that the company is planning for a world that must use less of its main product,\" LGIM said.LGIM\\'s Future World range of investments also includes the first fund aimed at encouraging gender diversity among UK firms, the Gender in Leadership UK Index Fund (GIRL), which was launched last month.Among the firms that LGIM considers leaders on climate change is Nestle, which has set targets to reduce greenhouse gases by 2020 in line with the Paris Agreement.The food giant discloses those targets and how it is performing against them.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "Much of what has been carried out here is not very efficient and would not be suitable for big data. During the course of finding the themes, the text was initally tokenized as part of the data cleaning, re-joined to find the TF-IDF which takes string inputs, and then tokenized again to find the top 100 words. This is what likely lead to 'n't' being treated as a word. Clearly, this isn't efficient and it also lead to bizarre behaviour, which thankfully was caught. In addition to this, nearly all methods involved looping over the large datasets. Running all the methods involves doing so multiple times. A much more efficient workflow must be devised with the complexity of each algorithm carefully considered and could ultilize features of NumPy to help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "BBC (2010) Section 4: Impartiality. in BBC _Editorial Guidelines_ London: BBC  http://downloads.bbc.co.uk/guidelines/editorialguidelines/pdfs/Section_04_Impartiality.pdf. Accessed 20<sup>th</sup> May 2018. <br />\n",
    "BBC (2018) _Our Values_ http://www.bbc.co.uk/careers/why-join-us/values. Accessed 17<sup>th</sup> June 2018. <br />\n",
    "BBC News (2017a) _Trump to BBC correspondent Jon Sopel: Here's another beauty_ https://www.bbc.co.uk/news/av/world-us-canada-38999996/trump-to-bbc-correspondent-jon-sopel-here-s-another-beauty Accessed 17<sup>th</sup> June 2018 <br />\n",
    "BBC News (2017b) _White House bans certain news media from briefing_ https://www.bbc.co.uk/news/world-us-canada-39085235 Accessed 17<sup>th</sup> June 2018 <br /> \n",
    "BBC Trust (2017) _Charter and Agreement_ http://www.bbc.co.uk/bbctrust/governance/regulatory_framework/charter_agreement.html. Accessed 17<sup>th</sup> June 2018 <br />\n",
    "Bramer, M. (2007) _Principles of Data Mining_. UK: Springer Science<br />\n",
    "Kolalapudi, S. (2016) _Understanding the Clustering Workflow_ [video]. https://app.pluralsight.com/library/courses/python-natural-language-processing/table-of-contents. Accessed 29<sup>th</sup> May 2018. <br />\n",
    "Pandas (2018) _Python Data Analysis Library_ https://pandas.pydata.org/ Accessed 17<sup>th</sup> June 2018 <br />\n",
    "SciKit Learn (2017) _sklearn.cluster.KMeans_. SciKit Learn Documentation. http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Accessed 15<sup>th</sup> June 2018. <br />\n",
    "Warburton, C., Hickman, L., Happer, C. and Jordan, D. (2018) _Why is Climate Change so Hard for News?_ [Seminar] BBC Salford Quays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
